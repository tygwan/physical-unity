# Autonomous Driving ML Platform

Unity ML-Agents ê¸°ë°˜ ììœ¨ì£¼í–‰ Motion Planning AI í•™ìŠµ í”Œë«í¼

> **Development Infrastructure**: This project uses [cc-initializer](https://github.com/tygwan/cc-initializer) for Claude Code workflow automation, including custom agents, skills, hooks, and development lifecycle management.

## Project Status

| Component | Status | Notes |
|-----------|--------|-------|
| Phase 1-2 | âœ… Complete | Foundation & Data Infrastructure |
| Phase 3-4 | â¸ï¸ On Hold | Ground Truth / Constant Velocity ì‚¬ìš© |
| **Phase 5** | ğŸ”„ **In Progress** | Planning Models (RL/IL) - PRIMARY FOCUS |
| Phase 6-7 | ğŸ“‹ Planned | Integration & Advanced Topics |

**Current Training**: Phase G (Intersection) í•™ìŠµ ì¤‘ - 340K steps, +461 reward

---

## Training History & Results

### Policy Evolution Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Early Experiments (Jan 22-24)                                                   â”‚
â”‚  3dball_test â†’ driving_ppo_v1 â†’ curriculum_v1~v9                                â”‚
â”‚       â”‚              â”‚               â”‚                                           â”‚
â”‚       â”‚              â”‚               â””â”€ Curriculum learning ê¸°ì´ˆ (reward shaping)â”‚
â”‚       â”‚              â””â”€ ì²« ììœ¨ì£¼í–‰ ì‹œë„ (ì‹¤íŒ¨: reward -4.9)                      â”‚
â”‚       â””â”€ ML-Agents í™˜ê²½ ê²€ì¦ (3D Ball: +100)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Main Training (Jan 24-27)                                                       â”‚
â”‚  v10g â†’ v11 â†’ v12 Phase A â†’ Phase B â†’ Phase C â†’ Phase D â†’ Phase E              â”‚
â”‚   â”‚      â”‚        â”‚           â”‚          â”‚          â”‚          â”‚                 â”‚
â”‚   â”‚      â”‚        â”‚           â”‚          â”‚          â”‚          â””â”€ ê³¡ì„  ë„ë¡œ      â”‚
â”‚   â”‚      â”‚        â”‚           â”‚          â”‚          â””â”€ Lane Observation (254D)  â”‚
â”‚   â”‚      â”‚        â”‚           â”‚          â””â”€ Multi-NPC Generalization (4 NPCs)   â”‚
â”‚   â”‚      â”‚        â”‚           â””â”€ Overtake vs Follow Decision                    â”‚
â”‚   â”‚      â”‚        â””â”€ Dense Overtaking (Slow NPC)                                â”‚
â”‚   â”‚      â””â”€ Sparse Overtaking Reward                                            â”‚
â”‚   â””â”€ Lane Keeping + NPC Coexistence                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Early Experiments (Pre-v10g)

| Run ID | Date | Steps | Reward | Purpose | Outcome |
|--------|------|-------|--------|---------|---------|
| 3dball_test5 | Jan 22 | 500K | **+100** | ML-Agents í™˜ê²½ ê²€ì¦ | âœ… íŠœí† ë¦¬ì–¼ ì„±ê³µ |
| driving_ppo_v1 | Jan 23 | 87K | -4.9 | ì²« ììœ¨ì£¼í–‰ ì‹œë„ | âŒ ê¸°ë³¸ ì£¼í–‰ ë¶ˆê°€ |
| curriculum_v1~v3 | Jan 24 | ~17K | - | Curriculum êµ¬ì¡° í…ŒìŠ¤íŠ¸ | âš ï¸ ì„¤ì • ì¡°ì • |
| curriculum_v4 | Jan 24 | 25K | - | Reward shaping ê°œì„  | âš ï¸ ìˆ˜ë ´ ë¶ˆì•ˆì • |
| curriculum_v5 | Jan 24 | 290K | **+275** | ì²« ì„±ê³µì  í•™ìŠµ | âœ… ê¸°ë³¸ ì£¼í–‰ ì„±ê³µ |
| curriculum_v6_parallel | Jan 24 | 2M | - | ë³‘ë ¬ í™˜ê²½ í…ŒìŠ¤íŠ¸ | âš ï¸ ì†ë„ í–¥ìƒ í™•ì¸ |
| curriculum_v7_speed | Jan 24 | 3.5M | -12 | Speed zone ë„ì… | âŒ ì†ë„ ì ì‘ ì‹¤íŒ¨ |
| curriculum_v8_gradual | Jan 24 | 285K | -3.4 | ì ì§„ì  ë‚œì´ë„ | âš ï¸ ê°œì„  í•„ìš” |
| curriculum_v9_speed | Jan 24 | - | - | Speed policy ê°œì„  | âš ï¸ v10 ì‹œë¦¬ì¦ˆë¡œ ì´ì–´ì§ |
| curriculum_v10a~f | Jan 24 | - | - | Traffic + NPC ì‹œë¦¬ì¦ˆ | âš ï¸ ë°˜ë³µ ê°œì„  |

### Main Training Results

| Phase | Base Checkpoint | Steps | Best Reward | Final Reward | Status | Key Achievement |
|-------|-----------------|-------|-------------|--------------|--------|-----------------|
| **Phase 0** | From scratch | 8M | **+1018** | +1018 | âœ… | Lane keeping, NPC coexistence |
| **Phase A** | Phase 0 | 2.5M | **+2113** | +2113 | âœ… | Overtaking mastery |
| **Phase B v1** | Phase 0 âš ï¸ | 3M | -108 | -108 | âŒ | **FAILED** - Wrong checkpoint + reward bug |
| **Phase B v2** | Phase A âœ… | 1M | **+877** | +877 | âœ… | Decision learning (recovery) |
| **Phase C** | Phase B v2 | 3.6M | **+1,372** | +1,372 | âœ… | Multi-NPC (8 NPCs), perfect safety |
| **Phase D v1** | Phase C | 6M | +406 | -2,156 | âŒ | **FAILED** - Curriculum collapse (3 params simultaneous) |
| **Phase D v2** | Phase C | TBD | TBD | TBD | ğŸ“‹ | Lane observation retry (single-param progression) |

**Legacy Results (Old Naming)**:
| Phase | Steps | Best Reward | Final Reward | Status | Key Achievement |
|-------|-------|-------------|--------------|--------|-----------------|
| v10g | 8M | +95 (NPC0) | +40 (NPC4) | âœ… | Lane keeping, NPC avoidance |
| v11 | 8M | +51 | +41 | âš ï¸ | Sparse reward insufficient |
| v12 Phase A (old) | 2M | +937 | +714 | âœ… | Learned overtaking maneuver |
| v12 Phase B (old) | 2M | +994 | +903 | âœ… | Overtake/follow decision |
| v12 Phase C (old) | 4M | +1086 | +961 | âœ… | 4-NPC generalization |
| v12 Phase D (old) | 6M | +402 | +332 | â­ï¸ | (Phase Eë¡œ ëŒ€ì²´) |
| v12 Phase E (old) | 6M | +931 | +931 | âœ… | Curved roads, 2 NPCs |
| v12 Phase F (old) | 6M | +988 | +988 | âœ… | Multi-lane roads |
| v12 Phase G (old) | 8M | +461 | ğŸ”„ | ğŸ”„ | Intersection navigation |
| v12_HybridPolicy | 3M | -82 | -2172 | âŒ | Catastrophic forgetting |

### Phase Details

#### v10g: Lane Keeping + NPC Coexistence
- **Intent**: Speed policy ê¸°ë°˜ ì£¼í–‰ + ì°¨ì„  ìœ ì§€
- **Problem**: Agent "follows" slow NPCs indefinitely (no overtaking)
- **Lesson**: `followingBonus` rewards "not crashing" - ì¶”ì›” ë™ê¸° ë¶€ì¬

#### v11: Sparse Overtaking Reward
- **Intent**: ëŠë¦° NPC ì¶”ì›” í•™ìŠµ (sparse reward)
- **Problem**: `targetSpeed = leadSpeed` êµ¬ì¡°ì  ë¬¸ì œ
- **Lesson**: Sparse rewardë§Œìœ¼ë¡œëŠ” ì¶”ì›” í•™ìŠµ ë¶ˆê°€

#### v12 Phase A: Dense Overtaking (Slow NPC)
- **Changes**:
  - `targetSpeed = speedLimit ALWAYS`
  - `followingBonus` ì œê±°
  - Dense 5-phase overtaking reward
- **Result**: +937 peak, ì¶”ì›” ë™ì‘ í•™ìŠµ ì„±ê³µ
- **Bug Fix**: Speed penalty ì¡°ê±´ë¬¸ ë²„ê·¸ ìˆ˜ì •

#### Phase B v1: Decision Learning (FAILED)
- **Start**: Phase 0 checkpoint (+1018) âš ï¸ Wrong choice
- **Environment**: 2 NPCs, decision-making curriculum
- **Duration**: 39.4 minutes, 3M steps
- **Result**: **-108 reward** (catastrophic failure)
- **Root Cause**:
  1. Reward bug: `followingPenalty` too harsh (-0.5/step)
  2. Wrong checkpoint: Phase 0 lacks overtaking capability
  3. Curriculum shock: 0 NPC â†’ 2 NPC too abrupt
- **Lesson**: Always resume from most capable checkpoint

#### Phase B v2: Decision Learning (Recovery SUCCESS)
- **Start**: Phase A checkpoint (2.5M steps, +2113) âœ… Correct
- **Curriculum**: 1â†’2â†’3â†’4 NPCs (gradual increase)
- **Duration**: ~1 hour, 1M additional steps (total 3.5M)
- **Result**: **+877 peak** (146% of target +600)
- **Improvement**:
  - Fixed reward function (removed harsh penalty)
  - Leveraged Phase A's overtaking capability
  - 4-stage curriculum prevented shock
- **Success Rate**: 100% goal completion, 0% collision

#### Phase C: Multi-NPC Generalization (4-8 NPCs)
- **Start**: Phase B v2 checkpoint (+877)
- **Environment**: 8 NPCs, complex multi-agent scenarios
- **Duration**: ~50 minutes, 3.6M steps
- **Result**: **+1,372 reward** (228% of target +600)
- **Achievement**:
  - Perfect safety (0% collision)
  - Robust generalization to 8 concurrent NPCs
  - Maintained high performance across complexity
- **Innovation**: Multi-agent decision-making at scale

#### Phase D v1: Lane Observation (FAILED)
- **Start**: Phase C checkpoint (+1,372)
- **Innovation**: Added 12D lane observation (242D â†’ 254D)
  - Explicit lane boundaries (left/right distances at 4 positions)
  - Faster convergence for lane-keeping
  - Preparation for curved roads (Phase E)
- **Duration**: 100 minutes, 6M steps
- **Peak**: **+406 at 4.6M steps** (promising start)
- **Collapse**: **-2,156 final** (catastrophic failure)
- **Root Cause**:
  1. 3 curriculum parameters transitioned simultaneously at 4.68M:
     - num_active_npcs: 1 â†’ 2
     - speed_zone_count: 1 â†’ 2
     - npc_speed_variation: 0 â†’ 0.3
  2. Agent's scenario-specific policies became invalid
  3. Collapse: +406 â†’ -4,825 in <20K steps (-5,231 points)
- **Lessons**:
  - Curriculum parameters are NOT independent
  - Peak reward â‰  robust learning
  - Simultaneous transitions = exponential complexity
- **Recovery**: Phase D v2 planned with single-parameter progression

#### v12 Phase C: Multi-NPC Generalization
- **Environment**: 1â†’2â†’3â†’4 NPCs, 230m goal, 4 speed zones
- **Curriculum Shock**: +766 â†’ -814 â†’ +1086 (recovery success)
- **Result**: +6% improvement in 4x complexity

#### v12 Phase D: Lane Observation (254D)
- **Changes**: 242D â†’ 254D (12D lane features added)
- **Curriculum**: 1â†’2 NPCs with curriculum shock recovery
- **Training**: 6M steps, +402 peak, +332 final
- **Result**: Successfully learned with expanded observation space

#### v12 Phase E: Curved Roads (Completed âœ…)
- **Goal**: ê³¡ì„  ë„ë¡œì—ì„œ ì•ˆì •ì  ì£¼í–‰ í•™ìŠµ
- **Results**: 6M steps, **+931 reward** (all curriculum passed)
- **Achievements**:
  - Sharp curves (curvature 1.0) ë§ˆìŠ¤í„°
  - Mixed left/right curve directions
  - 2 NPCs on curved roads
  - 200m goal distance on curves
- **Curriculum Completed**: Straight â†’ Gentle â†’ Moderate â†’ Sharp curves âœ…

#### v12 Phase F: Multi-Lane Roads (Completed âœ…)
- **Goal**: ë‹¤ì¤‘ ì°¨ì„  ë„ë¡œì—ì„œ ì£¼í–‰ í•™ìŠµ
- **Results**: 6M steps, **+988 reward** (all curriculum passed)
- **Achievements**:
  - 1â†’2 ì°¨ì„  ë„ë¡œ ë§ˆìŠ¤í„°
  - ì¤‘ì•™ì„  ê·œì¹™ í•™ìŠµ
  - ê³¡ì„  + ë‹¤ì°¨ì„  ë³µí•© í™˜ê²½
  - 3 NPCs on multi-lane roads
- **Curriculum Completed**: SingleLane â†’ TwoLanes â†’ CenterLine âœ…

#### v12 Phase G: Intersection Navigation (In Progress ğŸ”„)
- **Goal**: êµì°¨ë¡œ (Tì/ì‹­ì/Yì) ì£¼í–‰ í•™ìŠµ
- **Current**: 340K steps, **+461 reward**
- **Target**: 8M steps
- **Curriculum**: NoIntersection â†’ T-Junction â†’ Cross â†’ Y-Junction
- **Turn Direction**: Straight â†’ Left â†’ Right

#### v12_HybridPolicy: Incremental Learning Attempt (FAILED)
- **Goal**: Preserve Phase B knowledge while adding lane encoder
- **Method**: Freeze Phase B encoder, train new lane encoder
- **Failure**: Stage 5 (encoder fine-tuning) caused catastrophic forgetting
- **Lesson**: Don't unfreeze pretrained encoder even with low LR

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AUTONOMOUS DRIVING ML PLATFORM                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        Windows 11 Native                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   Unity 6 (6000.x) â”‚   â”‚    Python 3.10.11  â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚  â”‚ ML-Agents    â”‚  â”‚â—„â”€â–ºâ”‚  â”‚ PyTorch 2.3  â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚ 4.0.1        â”‚  â”‚   â”‚  â”‚ mlagents 1.2 â”‚  â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚  â”‚ Sentis 2.4.1 â”‚  â”‚   â”‚  â”‚ TensorBoard  â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚ ONNX Infer.  â”‚  â”‚   â”‚  â”‚ MLflow       â”‚  â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       OBSERVATION SPACE                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Ego (8D) â”‚ History (40D) â”‚ Agents (160D) â”‚ Route (30D) â”‚        â”‚â”‚
â”‚  â”‚ Speed (4D) â”‚ Lane (12D) â”‚ â†’ Total: 254D                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         ACTION SPACE                                 â”‚
â”‚  Steering: [-0.5, +0.5] rad   â”‚   Acceleration: [-4.0, +4.0] m/sÂ²   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Environment

| Component | Version | Notes |
|-----------|---------|-------|
| OS | Windows 11 | Native (WSL ë¯¸ì‚¬ìš©) |
| Unity | 6000.3.4f1 | Unity 6 LTS |
| ML-Agents | 4.0.1 | Unity Package |
| Sentis | 2.4.1 | ONNX Inference |
| Python | 3.10.11 | Windows Native |
| PyTorch | 2.3.1 | CUDA 12.x |
| mlagents | 1.2.0 | Python Package |
| GPU | RTX 4090 | 24GB VRAM |

---

## Project Structure

```
physical-unity/
â”œâ”€â”€ .claude/                    # Claude Code ì„¤ì •
â”œâ”€â”€ Assets/                     # Unity í”„ë¡œì íŠ¸
â”‚   â”œâ”€â”€ Scripts/
â”‚   â”‚   â”œâ”€â”€ Agents/            # E2EDrivingAgent.cs
â”‚   â”‚   â”œâ”€â”€ Environment/       # DrivingSceneManager.cs
â”‚   â”‚   â””â”€â”€ Sensors/           # CameraSensor, LiDARSensor
â”‚   â””â”€â”€ Resources/Models/      # ONNX ëª¨ë¸ íŒŒì¼
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PRD.md                 # ì œí’ˆ ìš”êµ¬ì‚¬í•­
â”‚   â”œâ”€â”€ TRAINING-LOG.md        # í•™ìŠµ ì‹¤í—˜ ê¸°ë¡ (ìƒì„¸)
â”‚   â”œâ”€â”€ LEARNING-ROADMAP.md    # RL/IL í•™ìŠµ ë¡œë“œë§µ
â”‚   â””â”€â”€ phases/                # Phaseë³„ ê¸°ìˆ  ë¬¸ì„œ
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ configs/planning/      # í•™ìŠµ ì„¤ì • YAML
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ models/            # PyTorch ëª¨ë¸
â”‚       â””â”€â”€ training/          # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ results/                   # í•™ìŠµ ê²°ê³¼ (TensorBoard)
â”‚   â”œâ”€â”€ phase-A/
â”‚   â”œâ”€â”€ phase-B/
â”‚   â”œâ”€â”€ phase-C/
â”‚   â””â”€â”€ v12_phaseD/
â””â”€â”€ models/planning/           # ìµœì¢… ONNX ëª¨ë¸
```

---

## Quick Start

### Training

```powershell
# Windows PowerShell
cd C:\Users\user\Desktop\dev\physical-unity

# Phase E í•™ìŠµ (í˜„ì¬ ì§„í–‰ì¤‘)
mlagents-learn python/configs/planning/vehicle_ppo_phase-E.yaml --run-id=phase-E

# Unity Editorì—ì„œ Play ë²„íŠ¼ í´ë¦­
```

### Monitoring

```powershell
# TensorBoard
tensorboard --logdir=results

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:6006 ì ‘ì†
```

### Inference (Unity)

1. `results/<run-id>/E2EDrivingAgent.onnx` â†’ `Assets/Resources/Models/` ë³µì‚¬
2. BehaviorParameters > Modelì— í• ë‹¹
3. BehaviorTypeì„ "Inference Only"ë¡œ ë³€ê²½
4. Play

---

## Reward Design (v12)

```yaml
# Per-step rewards
speed_compliance:     +0.3   # 80-100% of speed limit
speed_over_limit:     -0.5 ~ -3.0  # Progressive penalty
stuck_behind_npc:     -0.1   # After 3 seconds

# Overtaking rewards (one-time)
overtake_initiate:    +0.5   # Lane change started
overtake_beside:      +0.2/step  # Maintaining speed beside NPC
overtake_ahead:       +1.0   # Passed NPC
overtake_complete:    +2.0   # Returned to lane

# Penalties (one-time)
collision:            -5.0   # 3-strike rule
off_road:             -5.0   # Episode end
```

---

## Key Lessons Learned

### What Worked
1. **Dense Reward > Sparse Reward**: 5-phase overtaking reward enabled learning
2. **targetSpeed = speedLimit ALWAYS**: Critical for overtaking behavior
3. **Curriculum Learning**: Gradual complexity increase (NPC count, speed variation)
4. **Curriculum Shock Recovery**: Temporary drops are normal and recoverable
5. **Iterative Improvement**: v1 â†’ v10g ê³¼ì •ì—ì„œ ìˆ˜ì‹­ ë²ˆì˜ ì‹œí–‰ì°©ì˜¤ê°€ í•„ìˆ˜

### What Failed
1. **followingBonus**: Discouraged overtaking attempts
2. **Sparse overtakePassBonus**: Insufficient learning signal
3. **Hybrid Policy Encoder Fine-tuning**: Catastrophic forgetting at Stage 5
4. **ONNX Custom Format**: ML-Agents requires specific output names
5. **ê¸‰ê²©í•œ í™˜ê²½ ë³€í™”**: curriculum_v7ì—ì„œ speed zone ê°‘ìê¸° ë„ì… â†’ í•™ìŠµ ë¶•ê´´

### Early Phase Insights (Pre-v10g)
| Problem | Attempted | Result |
|---------|-----------|--------|
| "Agent doesn't move" | driving_ppo_v1 | Observation/Action ì—°ê²° ë¬¸ì œ |
| "Rewardê°€ ìˆ˜ë ´ ì•ˆë¨" | curriculum_v1~v4 | Reward shaping í•„ìš” |
| "í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼" | curriculum_v6_parallel | ë³‘ë ¬ í™˜ê²½ìœ¼ë¡œ 3x ì†ë„ í–¥ìƒ |
| "Speed zone ì ì‘ ì‹¤íŒ¨" | curriculum_v7_speed | ì ì§„ì  ë„ì… í•„ìš” (v10 ì‹œë¦¬ì¦ˆë¡œ í•´ê²°) |

### Best Practices
1. **Always verify observation dimensions**: BehaviorParameters Space Size = Agent output = ONNX input
2. **Monitor TensorBoard in real-time**: Catch issues early
3. **Save checkpoints frequently**: Best model may not be final model
4. **Don't unfreeze pretrained encoders**: Use very low LR or keep frozen

---

## Next Steps (Phase H+)

| Phase | Focus | Status |
|-------|-------|--------|
| **E** | ê³¡ì„  ë„ë¡œ + ë¹„ì •í˜• ê°ë„ | âœ… **Completed (+931)** |
| **F** | Nì°¨ì„  + ì¤‘ì•™ì„  ê·œì¹™ | âœ… **Completed (+988)** |
| **G** | êµì°¨ë¡œ (Tì/ì‹­ì/Yì) | ğŸ”„ **In Progress (340K, +461)** |
| **H** | ì‹ í˜¸ë“± + ì •ì§€ì„  | ğŸ“‹ Next |
| **I** | Uí„´ + íŠ¹ìˆ˜ ê¸°ë™ | ğŸ“‹ Planned |
| **J** | íš¡ë‹¨ë³´ë„ + ë³´í–‰ì | ğŸ“‹ Planned |
| **K** | ì¥ì• ë¬¼ + ê¸´ê¸‰ ìƒí™© | ğŸ“‹ Planned |
| **L** | ë³µí•© ì‹œë‚˜ë¦¬ì˜¤ í†µí•© | ğŸ“‹ Planned |

---

## Development Infrastructure

This project uses [cc-initializer](https://github.com/tygwan/cc-initializer) for automated development workflows with Claude Code.

### Agents (38)

**Core Framework Agents (26)**
| Category | Agents | Purpose |
|----------|--------|---------|
| **Documentation** | dev-docs-writer, doc-generator, doc-splitter, doc-validator, prd-writer, tech-spec-writer, readme-helper | ë¬¸ì„œ ìƒì„± ë° ê²€ì¦ |
| **Project Management** | progress-tracker, phase-tracker, project-analyzer, project-discovery, work-unit-manager | í”„ë¡œì íŠ¸ ì¶”ì  ë° ë¶„ì„ |
| **Code Quality** | code-reviewer, refactor-assistant, test-helper | ì½”ë“œ ë¦¬ë·° ë° í…ŒìŠ¤íŠ¸ |
| **Git/GitHub** | branch-manager, commit-helper, git-troubleshooter, github-manager, pr-creator | Git ì›Œí¬í”Œë¡œìš° ìë™í™” |
| **Analytics** | analytics-reporter, obsidian-sync | í†µê³„ ë° ì§€ì‹ ê´€ë¦¬ |
| **Infrastructure** | config-validator, file-explorer, google-searcher, agent-writer | ì¸í”„ë¼ ì§€ì› |

**ML/AD-Specific Agents (12)**
| Agent | Purpose | Trigger Keywords |
|-------|---------|------------------|
| ad-experiment-manager | AD ì‹¤í—˜ ìƒì„±, ì‹¤í–‰, ë¹„êµ, ì¶”ì  | "experiment", "ì‹¤í—˜", "training run", "í•™ìŠµ ì‹¤í–‰" |
| benchmark-evaluator | nuPlan ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰, ë©”íŠ¸ë¦­ ê³„ì‚° | "evaluate", "í‰ê°€", "benchmark", "metrics" |
| dataset-curator | ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ, ì „ì²˜ë¦¬, íë ˆì´ì…˜ | "dataset", "ë°ì´í„°ì…‹", "nuPlan", "Waymo" |
| experiment-documenter | ìë™ ì‹¤í—˜ ë¬¸ì„œí™” ë° ê²°ê³¼ ê¸°ë¡ | "ì‹¤í—˜ ë¬¸ì„œí™”", "í•™ìŠµ ì™„ë£Œ", "ê²°ê³¼ ê¸°ë¡", "update docs" |
| forensic-analyst | í•™ìŠµ ì‹¤íŒ¨ ê·¼ë³¸ ì›ì¸ ë¶„ì„ (ìˆ˜í•™ì  ê²€ì¦) | "ê·¼ë³¸ ì›ì¸", "root cause", "forensic", "ì™œ ì‹¤íŒ¨" |
| model-trainer | RL/IL í•™ìŠµ ì‹œì‘ ë° ê´€ë¦¬ | "train", "í•™ìŠµ", "PPO", "SAC", "GAIL" |
| training-analyst | í•™ìŠµ ê²°ê³¼ ë¶„ì„, ì„±ê³µ/ì‹¤íŒ¨ íŒì • | "ê²°ê³¼ ë¶„ì„", "ë¦¬í¬íŠ¸", "ì™œ ì‹¤íŒ¨", "ì›ì¸ ë¶„ì„" |
| training-doc-manager | í•™ìŠµ ë¬¸ì„œ ë™ê¸°í™”, ì•„ì¹´ì´ë¸Œ ê´€ë¦¬ | "ë¬¸ì„œ ë™ê¸°í™”", "ì•„ì¹´ì´ë¸Œ", "ë¡œê·¸ ì •ë¦¬" |
| training-monitor | ì‹¤ì‹œê°„ í•™ìŠµ ìƒíƒœ ëª¨ë‹ˆí„°ë§ | "í•™ìŠµ ìƒíƒœ", "ì§„í–‰ë¥ ", "ëª¨ë‹ˆí„°ë§", "í˜„ì¬ reward" |
| training-orchestrator | í•™ìŠµ ì›Œí¬í”Œë¡œìš° ì´ê´„ ì¡°ìœ¨ | "ë‹¤ìŒ ë‹¨ê³„", "ì›Œí¬í”Œë¡œìš°", "ì „ì²´ ìƒíƒœ" |
| training-planner | ì‹¤í—˜ ì„¤ê³„ ë° Config ìƒì„± | "ì‹¤í—˜ ì„¤ê³„", "ë‹¤ìŒ ë²„ì „", "config ìƒì„±" |
| training-site-publisher | GitHub Pages ì‚¬ì´íŠ¸ ë°œí–‰ | "ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸", "gh-pages", "ì›¹ ë°œí–‰" |

### Skills (22)

> **Note**: All core framework skills (18) from cc-initializer plus 4 ML-specific skills.

**Core Skills (18)**
| Skill | Description | Keywords |
|-------|-------------|----------|
| agile-sync | CHANGELOG, README, ì§„í–‰ìƒí™© ë™ê¸°í™” | "ë™ê¸°í™”", "sync", "changelog" |
| analytics | Tool/Agent ì‚¬ìš© í†µê³„ ì‹œê°í™” | "í†µê³„", "ì‚¬ìš©ëŸ‰", "analytics", "metrics" |
| brainstorming | ì•„ì´ë””ì–´ êµ¬ì²´í™” ë° ëŒ€ì•ˆ íƒìƒ‰ | "brainstorm", "ì•„ì´ë””ì–´", "alternative" |
| context-optimizer | ì»¨í…ìŠ¤íŠ¸ ë¡œë”© ìµœì í™” | "context", "token", "optimize", "summarize" |
| dev-doc-system | ê°œë°œ ë¬¸ì„œ í†µí•© ê´€ë¦¬ | "ë¬¸ì„œ ì‹œìŠ¤í…œ", "ê°œë°œ ê¸°ë¡", "ë°©í–¥ ì„¤ì •" |
| feedback-loop | í”¼ë“œë°± ìˆ˜ì§‘ ë° ADR ìƒì„± | "feedback", "learning", "retrospective" |
| gh | GitHub CLI í†µí•© | "github", "issue", "CI", "workflow" |
| hook-creator | Claude Code Hook ìƒì„± | "create hook", "configure hook" |
| obsidian | Obsidian vault ë™ê¸°í™” | "obsidian", "vault", "ì§€ì‹ ë™ê¸°í™”" |
| prompt-enhancer | í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í–¥ìƒ | "enhance prompt", "context-aware" |
| quality-gate | ê°œë°œ lifecycle Quality Gate | "pre-commit", "pre-merge", "quality" |
| readme-sync | README ìë™ ë™ê¸°í™” | "readme sync", "update readme" |
| repair | cc-initializer ìë™ ë³µêµ¬ | "repair", "fix", "troubleshoot" |
| skill-creator | ìƒˆë¡œìš´ Skill ìƒì„± ê°€ì´ë“œ | "create skill", "new skill" |
| sprint | Sprint lifecycle ê´€ë¦¬ | "sprint", "velocity", "burndown" |
| subagent-creator | ì»¤ìŠ¤í…€ Sub-agent ìƒì„± | "create agent", "new agent" |
| sync-fix | Phase/Sprint/ë¬¸ì„œ ë™ê¸°í™” ë¬¸ì œ í•´ê²° | "sync fix", "ë¶ˆì¼ì¹˜", "ë™ê¸°í™” ë¬¸ì œ" |
| validate | cc-initializer ì„¤ì • ê²€ì¦ | "validate", "ê²€ì¦", "check config" |

**ML-Specific Skills (4)**
| Skill | Description | Command |
|-------|-------------|---------|
| dataset | ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ, ì „ì²˜ë¦¬, ë¶„í•  | `/dataset` |
| experiment | ML ì‹¤í—˜ ìƒì„±, ì‹¤í–‰, ë¹„êµ, ì¶”ì  | `/experiment` |
| evaluate | ëª¨ë¸ í‰ê°€ ë° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ | `/evaluate` |
| train | RL/IL í•™ìŠµ ì‹œì‘ ë° ëª¨ë‹ˆí„°ë§ | `/train` |

### Commands (6)

| Command | Purpose | Integration |
|---------|---------|-------------|
| /bugfix | ë²„ê·¸ ìˆ˜ì • ì›Œí¬í”Œë¡œìš° (ì´ìŠˆ ë¶„ì„â†’PR) | Git + Phase + Sprint |
| /dev-doc-planner | PRD, ê¸°ìˆ  ì„¤ê³„ì„œ, ì§„í–‰ìƒí™© ë¬¸ì„œ ì‘ì„± | Templates (PRD/TECH-SPEC/PROGRESS) |
| /feature | ê¸°ëŠ¥ ê°œë°œ ì›Œí¬í”Œë¡œìš° (Phaseâ†’Sprintâ†’Gitâ†’Doc) | Phase + Sprint + Git + Docs |
| /git-workflow | Git ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ (ë¸Œëœì¹˜, ì»¤ë°‹, PR) | GitHub Flow + Conventional Commits |
| /phase | Phase ìƒíƒœ í™•ì¸, ì „í™˜, ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ | Phase ì‹œìŠ¤í…œ |
| /release | ë¦´ë¦¬ìŠ¤ ì›Œí¬í”Œë¡œìš° (ë²„ì „â†’ë¬¸ì„œâ†’ë°°í¬) | Git + Docs + Archive |

### Hooks (6)

**Pre-Tool Hooks**
- `pre-tool-use-safety.sh`: Bash/Write/Edit ì•ˆì „ì„± ê²€ì‚¬ (ìœ„í—˜ ëª…ë ¹ì–´ ì°¨ë‹¨)

**Post-Tool Hooks**
- `auto-doc-sync.sh`: Bash/Write/Edit í›„ ë¬¸ì„œ ìë™ ë™ê¸°í™”
- `phase-progress.sh`: Write/Edit í›„ Phase ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
- `post-tool-use-tracker.sh`: Bash/Write/Edit ì‚¬ìš© ì¶”ì  (analytics)

**Notification Hooks**
- `notification-handler.sh`: ëª¨ë“  ì•Œë¦¼ ì²˜ë¦¬

**Utility Hooks**
- `error-recovery.sh`: Hook ì‹¤íŒ¨ ì‹œ ìë™ ë³µêµ¬

### Key Features

**Automation**
- Phase/Sprint ìë™ ì§„í–‰ ì¶”ì 
- Git ì›Œí¬í”Œë¡œìš° ìë™í™” (Conventional Commits)
- ë¬¸ì„œ ìë™ ë™ê¸°í™” (CHANGELOG, README, PROGRESS)
- Quality Gate (pre-commit, pre-merge, pre-release)

**ML/AD Specific**
- ì‹¤í—˜ ì¶”ì  ë° ë¹„êµ (MLflow/W&B í†µí•©)
- TensorBoard ëª¨ë‹ˆí„°ë§
- í•™ìŠµ ë¬¸ì„œ ìë™ ìƒì„± ë° ì•„ì¹´ì´ë¸Œ
- GitHub Pages ìë™ ë°œí–‰

**Safety & Recovery**
- ìœ„í—˜ ëª…ë ¹ì–´ ì°¨ë‹¨ (rm -rf, git reset --hard ë“±)
- Hook ì‹¤íŒ¨ ì‹œ ìë™ ë³µêµ¬
- ë¬¸ì„œ ì†ìƒ ê°ì§€ ë° ë³µêµ¬

**Analytics**
- Tool/Agent/Skill ì‚¬ìš© í†µê³„ (JSONL)
- CLI ì°¨íŠ¸ ì‹œê°í™”
- 30ì¼ ë°ì´í„° ë³´ê´€

---

## Documentation

- [PRD (Product Requirements)](docs/PRD.md)
- [Training Log (Detailed)](docs/TRAINING-LOG.md)
- [Learning Roadmap](docs/LEARNING-ROADMAP.md)
- [Phase Documents](docs/phases/README.md)
- [Progress Tracking](docs/PROGRESS.md)
- [Workflow Diagrams (Mermaid)](docs/WORKFLOW-DIAGRAMS.md)
- [cc-initializer Components](.claude/docs/CC-INITIALIZER-COMPONENTS.md)

---

## References

- [Unity ML-Agents Documentation](https://unity-technologies.github.io/ml-agents/)
- [ML-Agents GitHub](https://github.com/Unity-Technologies/ml-agents)
- [PPO Algorithm](https://arxiv.org/abs/1707.06347)

---

**Last Updated**: 2026-01-29 | **Phase G In Progress** | Phase F: +988, Phase G: +461 (340K)

**Development Infrastructure**: [cc-initializer](https://github.com/tygwan/cc-initializer) - 38 Agents, 29 Skills, 6 Hooks, 12 Commands
