# Autonomous Driving ML Platform

Unity ML-Agents ê¸°ë°˜ ììœ¨ì£¼í–‰ Motion Planning AI í•™ìŠµ í”Œë«í¼

## Project Status

| Component | Status | Notes |
|-----------|--------|-------|
| Phase 1-2 | âœ… Complete | Foundation & Data Infrastructure |
| Phase 3-4 | â¸ï¸ On Hold | Ground Truth / Constant Velocity ì‚¬ìš© |
| **Phase 5** | ğŸ”„ **In Progress** | Planning Models (RL/IL) - PRIMARY FOCUS |
| Phase 6-7 | ğŸ“‹ Planned | Integration & Advanced Topics |

**Current Training**: Phase D Complete (Lane Observation 254D)

---

## Training History & Results

### Policy Evolution Summary

```
v10g â†’ v11 â†’ v12 Phase A â†’ Phase B â†’ Phase C â†’ Phase D
 â”‚      â”‚        â”‚           â”‚          â”‚          â”‚
 â”‚      â”‚        â”‚           â”‚          â”‚          â””â”€ Lane Observation (254D)
 â”‚      â”‚        â”‚           â”‚          â””â”€ Multi-NPC Generalization (4 NPCs)
 â”‚      â”‚        â”‚           â””â”€ Overtake vs Follow Decision
 â”‚      â”‚        â””â”€ Dense Overtaking (Slow NPC)
 â”‚      â””â”€ Sparse Overtaking Reward
 â””â”€ Lane Keeping + NPC Coexistence
```

### Results by Phase

| Phase | Steps | Best Reward | Final Reward | Status | Key Achievement |
|-------|-------|-------------|--------------|--------|-----------------|
| v10g | 8M | +95 (NPC0) | +40 (NPC4) | âœ… | Lane keeping, NPC avoidance |
| v11 | 8M | +51 | +41 | âš ï¸ | Sparse reward insufficient |
| **v12 Phase A** | 2M | **+937** | +714 | âœ… | Learned overtaking maneuver |
| **v12 Phase B** | 2M | **+994** | +903 | âœ… | Overtake/follow decision |
| **v12 Phase C** | 4M | **+1086** | +961 | âœ… | 4-NPC generalization |
| **v12 Phase D** | 6M | **+402** | +332 | âœ… | Lane observation (254D) |
| v12_HybridPolicy | 3M | -82 | -2172 | âŒ | Catastrophic forgetting |

### Phase Details

#### v10g: Lane Keeping + NPC Coexistence
- **Intent**: Speed policy ê¸°ë°˜ ì£¼í–‰ + ì°¨ì„  ìœ ì§€
- **Problem**: Agent "follows" slow NPCs indefinitely (no overtaking)
- **Lesson**: `followingBonus` rewards "not crashing" - ì¶”ì›” ë™ê¸° ë¶€ì¬

#### v11: Sparse Overtaking Reward
- **Intent**: ëŠë¦° NPC ì¶”ì›” í•™ìŠµ (sparse reward)
- **Problem**: `targetSpeed = leadSpeed` êµ¬ì¡°ì  ë¬¸ì œ
- **Lesson**: Sparse rewardë§Œìœ¼ë¡œëŠ” ì¶”ì›” í•™ìŠµ ë¶ˆê°€

#### v12 Phase A: Dense Overtaking (Slow NPC)
- **Changes**:
  - `targetSpeed = speedLimit ALWAYS`
  - `followingBonus` ì œê±°
  - Dense 5-phase overtaking reward
- **Result**: +937 peak, ì¶”ì›” ë™ì‘ í•™ìŠµ ì„±ê³µ
- **Bug Fix**: Speed penalty ì¡°ê±´ë¬¸ ë²„ê·¸ ìˆ˜ì •

#### v12 Phase B: Overtake vs Follow Decision
- **Curriculum**: NPC speed 0.3 â†’ 0.5 â†’ 0.7 â†’ 0.9
- **Result**: +994 peak, ì¡°ê±´ë¶€ ì¶”ì›”/ë”°ë¼ê°€ê¸° íŒë‹¨ í•™ìŠµ
- **Improvement**: +26% over Phase A

#### v12 Phase C: Multi-NPC Generalization
- **Environment**: 1â†’2â†’3â†’4 NPCs, 230m goal, 4 speed zones
- **Curriculum Shock**: +766 â†’ -814 â†’ +1086 (recovery success)
- **Result**: +6% improvement in 4x complexity

#### v12 Phase D: Lane Observation (254D)
- **Changes**: 242D â†’ 254D (12D lane features added)
- **Curriculum**: 1â†’2 NPCs with curriculum shock recovery
- **Training**: 6M steps, +402 peak, +332 final
- **Result**: Successfully learned with expanded observation space

#### v12_HybridPolicy: Incremental Learning Attempt (FAILED)
- **Goal**: Preserve Phase B knowledge while adding lane encoder
- **Method**: Freeze Phase B encoder, train new lane encoder
- **Failure**: Stage 5 (encoder fine-tuning) caused catastrophic forgetting
- **Lesson**: Don't unfreeze pretrained encoder even with low LR

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AUTONOMOUS DRIVING ML PLATFORM                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        Windows 11 Native                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   Unity 6 (6000.x) â”‚   â”‚    Python 3.10.11  â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚  â”‚ ML-Agents    â”‚  â”‚â—„â”€â–ºâ”‚  â”‚ PyTorch 2.3  â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚ 4.0.1        â”‚  â”‚   â”‚  â”‚ mlagents 1.2 â”‚  â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚  â”‚ Sentis 2.4.1 â”‚  â”‚   â”‚  â”‚ TensorBoard  â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚ ONNX Infer.  â”‚  â”‚   â”‚  â”‚ MLflow       â”‚  â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       OBSERVATION SPACE                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Ego (8D) â”‚ History (40D) â”‚ Agents (160D) â”‚ Route (30D) â”‚        â”‚â”‚
â”‚  â”‚ Speed (4D) â”‚ Lane (12D) â”‚ â†’ Total: 254D                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         ACTION SPACE                                 â”‚
â”‚  Steering: [-0.5, +0.5] rad   â”‚   Acceleration: [-4.0, +4.0] m/sÂ²   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Environment

| Component | Version | Notes |
|-----------|---------|-------|
| OS | Windows 11 | Native (WSL ë¯¸ì‚¬ìš©) |
| Unity | 6000.3.4f1 | Unity 6 LTS |
| ML-Agents | 4.0.1 | Unity Package |
| Sentis | 2.4.1 | ONNX Inference |
| Python | 3.10.11 | Windows Native |
| PyTorch | 2.3.1 | CUDA 12.x |
| mlagents | 1.2.0 | Python Package |
| GPU | RTX 4090 | 24GB VRAM |

---

## Project Structure

```
physical-unity/
â”œâ”€â”€ .claude/                    # Claude Code ì„¤ì •
â”œâ”€â”€ Assets/                     # Unity í”„ë¡œì íŠ¸
â”‚   â”œâ”€â”€ Scripts/
â”‚   â”‚   â”œâ”€â”€ Agents/            # E2EDrivingAgent.cs
â”‚   â”‚   â”œâ”€â”€ Environment/       # DrivingSceneManager.cs
â”‚   â”‚   â””â”€â”€ Sensors/           # CameraSensor, LiDARSensor
â”‚   â””â”€â”€ Resources/Models/      # ONNX ëª¨ë¸ íŒŒì¼
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PRD.md                 # ì œí’ˆ ìš”êµ¬ì‚¬í•­
â”‚   â”œâ”€â”€ TRAINING-LOG.md        # í•™ìŠµ ì‹¤í—˜ ê¸°ë¡ (ìƒì„¸)
â”‚   â”œâ”€â”€ LEARNING-ROADMAP.md    # RL/IL í•™ìŠµ ë¡œë“œë§µ
â”‚   â””â”€â”€ phases/                # Phaseë³„ ê¸°ìˆ  ë¬¸ì„œ
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ configs/planning/      # í•™ìŠµ ì„¤ì • YAML
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ models/            # PyTorch ëª¨ë¸
â”‚       â””â”€â”€ training/          # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ results/                   # í•™ìŠµ ê²°ê³¼ (TensorBoard)
â”‚   â”œâ”€â”€ v12_phaseA/
â”‚   â”œâ”€â”€ v12_phaseB/
â”‚   â”œâ”€â”€ v12_phaseC/
â”‚   â””â”€â”€ v12_phaseD/
â””â”€â”€ models/planning/           # ìµœì¢… ONNX ëª¨ë¸
```

---

## Quick Start

### Training

```powershell
# Windows PowerShell
cd C:\Users\user\Desktop\dev\physical-unity

# Phase D í•™ìŠµ (ì˜ˆì‹œ)
mlagents-learn python/configs/planning/vehicle_ppo_v12_phaseD.yaml --run-id=v12_phaseD

# Unity Editorì—ì„œ Play ë²„íŠ¼ í´ë¦­
```

### Monitoring

```powershell
# TensorBoard
tensorboard --logdir=results

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:6006 ì ‘ì†
```

### Inference (Unity)

1. `results/<run-id>/E2EDrivingAgent.onnx` â†’ `Assets/Resources/Models/` ë³µì‚¬
2. BehaviorParameters > Modelì— í• ë‹¹
3. BehaviorTypeì„ "Inference Only"ë¡œ ë³€ê²½
4. Play

---

## Reward Design (v12)

```yaml
# Per-step rewards
speed_compliance:     +0.3   # 80-100% of speed limit
speed_over_limit:     -0.5 ~ -3.0  # Progressive penalty
stuck_behind_npc:     -0.1   # After 3 seconds

# Overtaking rewards (one-time)
overtake_initiate:    +0.5   # Lane change started
overtake_beside:      +0.2/step  # Maintaining speed beside NPC
overtake_ahead:       +1.0   # Passed NPC
overtake_complete:    +2.0   # Returned to lane

# Penalties (one-time)
collision:            -5.0   # 3-strike rule
off_road:             -5.0   # Episode end
```

---

## Key Lessons Learned

### What Worked
1. **Dense Reward > Sparse Reward**: 5-phase overtaking reward enabled learning
2. **targetSpeed = speedLimit ALWAYS**: Critical for overtaking behavior
3. **Curriculum Learning**: Gradual complexity increase (NPC count, speed variation)
4. **Curriculum Shock Recovery**: Temporary drops are normal and recoverable

### What Failed
1. **followingBonus**: Discouraged overtaking attempts
2. **Sparse overtakePassBonus**: Insufficient learning signal
3. **Hybrid Policy Encoder Fine-tuning**: Catastrophic forgetting at Stage 5
4. **ONNX Custom Format**: ML-Agents requires specific output names

### Best Practices
1. **Always verify observation dimensions**: BehaviorParameters Space Size = Agent output = ONNX input
2. **Monitor TensorBoard in real-time**: Catch issues early
3. **Save checkpoints frequently**: Best model may not be final model
4. **Don't unfreeze pretrained encoders**: Use very low LR or keep frozen

---

## Next Steps (Phase E+)

| Phase | Focus | Status |
|-------|-------|--------|
| **E** | ê³¡ì„  ë„ë¡œ + ë¹„ì •í˜• ê°ë„ | ğŸ“‹ Planned |
| **F** | Nì°¨ì„  + ì¤‘ì•™ì„  ê·œì¹™ | ğŸ“‹ Planned |
| **G** | êµì°¨ë¡œ (Tì/ì‹­ì/Yì) | ğŸ“‹ Planned |
| **H** | ì‹ í˜¸ë“± + ì •ì§€ì„  | ğŸ“‹ Planned |
| **I** | Uí„´ + íŠ¹ìˆ˜ ê¸°ë™ | ğŸ“‹ Planned |
| **J** | íš¡ë‹¨ë³´ë„ + ë³´í–‰ì | ğŸ“‹ Planned |
| **K** | ì¥ì• ë¬¼ + ê¸´ê¸‰ ìƒí™© | ğŸ“‹ Planned |
| **L** | ë³µí•© ì‹œë‚˜ë¦¬ì˜¤ í†µí•© | ğŸ“‹ Planned |

---

## Documentation

- [PRD (Product Requirements)](docs/PRD.md)
- [Training Log (Detailed)](docs/TRAINING-LOG.md)
- [Learning Roadmap](docs/LEARNING-ROADMAP.md)
- [Phase Documents](docs/phases/README.md)
- [Progress Tracking](docs/PROGRESS.md)

---

## References

- [Unity ML-Agents Documentation](https://unity-technologies.github.io/ml-agents/)
- [ML-Agents GitHub](https://github.com/Unity-Technologies/ml-agents)
- [PPO Algorithm](https://arxiv.org/abs/1707.06347)

---

**Last Updated**: 2026-01-27 | **Phase D Complete** | Reward: +332
