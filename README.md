# Autonomous Driving ML Platform

Unity ML-Agents ê¸°ë°˜ ììœ¨ì£¼í–‰ Motion Planning AI í•™ìŠµ í”Œë«í¼

## Project Status

| Component | Status | Notes |
|-----------|--------|-------|
| Phase 1-2 | âœ… Complete | Foundation & Data Infrastructure |
| Phase 3-4 | â¸ï¸ On Hold | Ground Truth / Constant Velocity ì‚¬ìš© |
| **Phase 5** | ğŸ”„ **In Progress** | Planning Models (RL/IL) - PRIMARY FOCUS |
| Phase 6-7 | ğŸ“‹ Planned | Integration & Advanced Topics |

**Current Training**: Phase E Completed (+931), Phase F Next (Nì°¨ì„ )

---

## Training History & Results

### Policy Evolution Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Early Experiments (Jan 22-24)                                                   â”‚
â”‚  3dball_test â†’ driving_ppo_v1 â†’ curriculum_v1~v9                                â”‚
â”‚       â”‚              â”‚               â”‚                                           â”‚
â”‚       â”‚              â”‚               â””â”€ Curriculum learning ê¸°ì´ˆ (reward shaping)â”‚
â”‚       â”‚              â””â”€ ì²« ììœ¨ì£¼í–‰ ì‹œë„ (ì‹¤íŒ¨: reward -4.9)                      â”‚
â”‚       â””â”€ ML-Agents í™˜ê²½ ê²€ì¦ (3D Ball: +100)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Main Training (Jan 24-27)                                                       â”‚
â”‚  v10g â†’ v11 â†’ v12 Phase A â†’ Phase B â†’ Phase C â†’ Phase D â†’ Phase E              â”‚
â”‚   â”‚      â”‚        â”‚           â”‚          â”‚          â”‚          â”‚                 â”‚
â”‚   â”‚      â”‚        â”‚           â”‚          â”‚          â”‚          â””â”€ ê³¡ì„  ë„ë¡œ      â”‚
â”‚   â”‚      â”‚        â”‚           â”‚          â”‚          â””â”€ Lane Observation (254D)  â”‚
â”‚   â”‚      â”‚        â”‚           â”‚          â””â”€ Multi-NPC Generalization (4 NPCs)   â”‚
â”‚   â”‚      â”‚        â”‚           â””â”€ Overtake vs Follow Decision                    â”‚
â”‚   â”‚      â”‚        â””â”€ Dense Overtaking (Slow NPC)                                â”‚
â”‚   â”‚      â””â”€ Sparse Overtaking Reward                                            â”‚
â”‚   â””â”€ Lane Keeping + NPC Coexistence                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Early Experiments (Pre-v10g)

| Run ID | Date | Steps | Reward | Purpose | Outcome |
|--------|------|-------|--------|---------|---------|
| 3dball_test5 | Jan 22 | 500K | **+100** | ML-Agents í™˜ê²½ ê²€ì¦ | âœ… íŠœí† ë¦¬ì–¼ ì„±ê³µ |
| driving_ppo_v1 | Jan 23 | 87K | -4.9 | ì²« ììœ¨ì£¼í–‰ ì‹œë„ | âŒ ê¸°ë³¸ ì£¼í–‰ ë¶ˆê°€ |
| curriculum_v1~v3 | Jan 24 | ~17K | - | Curriculum êµ¬ì¡° í…ŒìŠ¤íŠ¸ | âš ï¸ ì„¤ì • ì¡°ì • |
| curriculum_v4 | Jan 24 | 25K | - | Reward shaping ê°œì„  | âš ï¸ ìˆ˜ë ´ ë¶ˆì•ˆì • |
| curriculum_v5 | Jan 24 | 290K | **+275** | ì²« ì„±ê³µì  í•™ìŠµ | âœ… ê¸°ë³¸ ì£¼í–‰ ì„±ê³µ |
| curriculum_v6_parallel | Jan 24 | 2M | - | ë³‘ë ¬ í™˜ê²½ í…ŒìŠ¤íŠ¸ | âš ï¸ ì†ë„ í–¥ìƒ í™•ì¸ |
| curriculum_v7_speed | Jan 24 | 3.5M | -12 | Speed zone ë„ì… | âŒ ì†ë„ ì ì‘ ì‹¤íŒ¨ |
| curriculum_v8_gradual | Jan 24 | 285K | -3.4 | ì ì§„ì  ë‚œì´ë„ | âš ï¸ ê°œì„  í•„ìš” |
| curriculum_v9_speed | Jan 24 | - | - | Speed policy ê°œì„  | âš ï¸ v10 ì‹œë¦¬ì¦ˆë¡œ ì´ì–´ì§ |
| curriculum_v10a~f | Jan 24 | - | - | Traffic + NPC ì‹œë¦¬ì¦ˆ | âš ï¸ ë°˜ë³µ ê°œì„  |

### Main Training Results

| Phase | Steps | Best Reward | Final Reward | Status | Key Achievement |
|-------|-------|-------------|--------------|--------|-----------------|
| v10g | 8M | +95 (NPC0) | +40 (NPC4) | âœ… | Lane keeping, NPC avoidance |
| v11 | 8M | +51 | +41 | âš ï¸ | Sparse reward insufficient |
| **v12 Phase A** | 2M | **+937** | +714 | âœ… | Learned overtaking maneuver |
| **v12 Phase B** | 2M | **+994** | +903 | âœ… | Overtake/follow decision |
| **v12 Phase C** | 4M | **+1086** | +961 | âœ… | 4-NPC generalization |
| **v12 Phase D** | 6M | **+402** | +332 | âœ… | Lane observation (254D) |
| **v12 Phase E** | 6M | **+931** | +931 | âœ… | Curved roads, 2 NPCs |
| v12_HybridPolicy | 3M | -82 | -2172 | âŒ | Catastrophic forgetting |

### Phase Details

#### v10g: Lane Keeping + NPC Coexistence
- **Intent**: Speed policy ê¸°ë°˜ ì£¼í–‰ + ì°¨ì„  ìœ ì§€
- **Problem**: Agent "follows" slow NPCs indefinitely (no overtaking)
- **Lesson**: `followingBonus` rewards "not crashing" - ì¶”ì›” ë™ê¸° ë¶€ì¬

#### v11: Sparse Overtaking Reward
- **Intent**: ëŠë¦° NPC ì¶”ì›” í•™ìŠµ (sparse reward)
- **Problem**: `targetSpeed = leadSpeed` êµ¬ì¡°ì  ë¬¸ì œ
- **Lesson**: Sparse rewardë§Œìœ¼ë¡œëŠ” ì¶”ì›” í•™ìŠµ ë¶ˆê°€

#### v12 Phase A: Dense Overtaking (Slow NPC)
- **Changes**:
  - `targetSpeed = speedLimit ALWAYS`
  - `followingBonus` ì œê±°
  - Dense 5-phase overtaking reward
- **Result**: +937 peak, ì¶”ì›” ë™ì‘ í•™ìŠµ ì„±ê³µ
- **Bug Fix**: Speed penalty ì¡°ê±´ë¬¸ ë²„ê·¸ ìˆ˜ì •

#### v12 Phase B: Overtake vs Follow Decision
- **Curriculum**: NPC speed 0.3 â†’ 0.5 â†’ 0.7 â†’ 0.9
- **Result**: +994 peak, ì¡°ê±´ë¶€ ì¶”ì›”/ë”°ë¼ê°€ê¸° íŒë‹¨ í•™ìŠµ
- **Improvement**: +26% over Phase A

#### v12 Phase C: Multi-NPC Generalization
- **Environment**: 1â†’2â†’3â†’4 NPCs, 230m goal, 4 speed zones
- **Curriculum Shock**: +766 â†’ -814 â†’ +1086 (recovery success)
- **Result**: +6% improvement in 4x complexity

#### v12 Phase D: Lane Observation (254D)
- **Changes**: 242D â†’ 254D (12D lane features added)
- **Curriculum**: 1â†’2 NPCs with curriculum shock recovery
- **Training**: 6M steps, +402 peak, +332 final
- **Result**: Successfully learned with expanded observation space

#### v12 Phase E: Curved Roads (Completed âœ…)
- **Goal**: ê³¡ì„  ë„ë¡œì—ì„œ ì•ˆì •ì  ì£¼í–‰ í•™ìŠµ
- **Results**: 6M steps, **+931 reward** (all curriculum passed)
- **Achievements**:
  - Sharp curves (curvature 1.0) ë§ˆìŠ¤í„°
  - Mixed left/right curve directions
  - 2 NPCs on curved roads
  - 200m goal distance on curves
- **Curriculum Completed**: Straight â†’ Gentle â†’ Moderate â†’ Sharp curves âœ…

#### v12_HybridPolicy: Incremental Learning Attempt (FAILED)
- **Goal**: Preserve Phase B knowledge while adding lane encoder
- **Method**: Freeze Phase B encoder, train new lane encoder
- **Failure**: Stage 5 (encoder fine-tuning) caused catastrophic forgetting
- **Lesson**: Don't unfreeze pretrained encoder even with low LR

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AUTONOMOUS DRIVING ML PLATFORM                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        Windows 11 Native                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   Unity 6 (6000.x) â”‚   â”‚    Python 3.10.11  â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚  â”‚ ML-Agents    â”‚  â”‚â—„â”€â–ºâ”‚  â”‚ PyTorch 2.3  â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚ 4.0.1        â”‚  â”‚   â”‚  â”‚ mlagents 1.2 â”‚  â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚  â”‚ Sentis 2.4.1 â”‚  â”‚   â”‚  â”‚ TensorBoard  â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚ ONNX Infer.  â”‚  â”‚   â”‚  â”‚ MLflow       â”‚  â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       OBSERVATION SPACE                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Ego (8D) â”‚ History (40D) â”‚ Agents (160D) â”‚ Route (30D) â”‚        â”‚â”‚
â”‚  â”‚ Speed (4D) â”‚ Lane (12D) â”‚ â†’ Total: 254D                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         ACTION SPACE                                 â”‚
â”‚  Steering: [-0.5, +0.5] rad   â”‚   Acceleration: [-4.0, +4.0] m/sÂ²   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Environment

| Component | Version | Notes |
|-----------|---------|-------|
| OS | Windows 11 | Native (WSL ë¯¸ì‚¬ìš©) |
| Unity | 6000.3.4f1 | Unity 6 LTS |
| ML-Agents | 4.0.1 | Unity Package |
| Sentis | 2.4.1 | ONNX Inference |
| Python | 3.10.11 | Windows Native |
| PyTorch | 2.3.1 | CUDA 12.x |
| mlagents | 1.2.0 | Python Package |
| GPU | RTX 4090 | 24GB VRAM |

---

## Project Structure

```
physical-unity/
â”œâ”€â”€ .claude/                    # Claude Code ì„¤ì •
â”œâ”€â”€ Assets/                     # Unity í”„ë¡œì íŠ¸
â”‚   â”œâ”€â”€ Scripts/
â”‚   â”‚   â”œâ”€â”€ Agents/            # E2EDrivingAgent.cs
â”‚   â”‚   â”œâ”€â”€ Environment/       # DrivingSceneManager.cs
â”‚   â”‚   â””â”€â”€ Sensors/           # CameraSensor, LiDARSensor
â”‚   â””â”€â”€ Resources/Models/      # ONNX ëª¨ë¸ íŒŒì¼
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PRD.md                 # ì œí’ˆ ìš”êµ¬ì‚¬í•­
â”‚   â”œâ”€â”€ TRAINING-LOG.md        # í•™ìŠµ ì‹¤í—˜ ê¸°ë¡ (ìƒì„¸)
â”‚   â”œâ”€â”€ LEARNING-ROADMAP.md    # RL/IL í•™ìŠµ ë¡œë“œë§µ
â”‚   â””â”€â”€ phases/                # Phaseë³„ ê¸°ìˆ  ë¬¸ì„œ
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ configs/planning/      # í•™ìŠµ ì„¤ì • YAML
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ models/            # PyTorch ëª¨ë¸
â”‚       â””â”€â”€ training/          # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ results/                   # í•™ìŠµ ê²°ê³¼ (TensorBoard)
â”‚   â”œâ”€â”€ v12_phaseA/
â”‚   â”œâ”€â”€ v12_phaseB/
â”‚   â”œâ”€â”€ v12_phaseC/
â”‚   â””â”€â”€ v12_phaseD/
â””â”€â”€ models/planning/           # ìµœì¢… ONNX ëª¨ë¸
```

---

## Quick Start

### Training

```powershell
# Windows PowerShell
cd C:\Users\user\Desktop\dev\physical-unity

# Phase E í•™ìŠµ (í˜„ì¬ ì§„í–‰ì¤‘)
mlagents-learn python/configs/planning/vehicle_ppo_v12_phaseE.yaml --run-id=v12_phaseE

# Unity Editorì—ì„œ Play ë²„íŠ¼ í´ë¦­
```

### Monitoring

```powershell
# TensorBoard
tensorboard --logdir=results

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:6006 ì ‘ì†
```

### Inference (Unity)

1. `results/<run-id>/E2EDrivingAgent.onnx` â†’ `Assets/Resources/Models/` ë³µì‚¬
2. BehaviorParameters > Modelì— í• ë‹¹
3. BehaviorTypeì„ "Inference Only"ë¡œ ë³€ê²½
4. Play

---

## Reward Design (v12)

```yaml
# Per-step rewards
speed_compliance:     +0.3   # 80-100% of speed limit
speed_over_limit:     -0.5 ~ -3.0  # Progressive penalty
stuck_behind_npc:     -0.1   # After 3 seconds

# Overtaking rewards (one-time)
overtake_initiate:    +0.5   # Lane change started
overtake_beside:      +0.2/step  # Maintaining speed beside NPC
overtake_ahead:       +1.0   # Passed NPC
overtake_complete:    +2.0   # Returned to lane

# Penalties (one-time)
collision:            -5.0   # 3-strike rule
off_road:             -5.0   # Episode end
```

---

## Key Lessons Learned

### What Worked
1. **Dense Reward > Sparse Reward**: 5-phase overtaking reward enabled learning
2. **targetSpeed = speedLimit ALWAYS**: Critical for overtaking behavior
3. **Curriculum Learning**: Gradual complexity increase (NPC count, speed variation)
4. **Curriculum Shock Recovery**: Temporary drops are normal and recoverable
5. **Iterative Improvement**: v1 â†’ v10g ê³¼ì •ì—ì„œ ìˆ˜ì‹­ ë²ˆì˜ ì‹œí–‰ì°©ì˜¤ê°€ í•„ìˆ˜

### What Failed
1. **followingBonus**: Discouraged overtaking attempts
2. **Sparse overtakePassBonus**: Insufficient learning signal
3. **Hybrid Policy Encoder Fine-tuning**: Catastrophic forgetting at Stage 5
4. **ONNX Custom Format**: ML-Agents requires specific output names
5. **ê¸‰ê²©í•œ í™˜ê²½ ë³€í™”**: curriculum_v7ì—ì„œ speed zone ê°‘ìê¸° ë„ì… â†’ í•™ìŠµ ë¶•ê´´

### Early Phase Insights (Pre-v10g)
| Problem | Attempted | Result |
|---------|-----------|--------|
| "Agent doesn't move" | driving_ppo_v1 | Observation/Action ì—°ê²° ë¬¸ì œ |
| "Rewardê°€ ìˆ˜ë ´ ì•ˆë¨" | curriculum_v1~v4 | Reward shaping í•„ìš” |
| "í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼" | curriculum_v6_parallel | ë³‘ë ¬ í™˜ê²½ìœ¼ë¡œ 3x ì†ë„ í–¥ìƒ |
| "Speed zone ì ì‘ ì‹¤íŒ¨" | curriculum_v7_speed | ì ì§„ì  ë„ì… í•„ìš” (v10 ì‹œë¦¬ì¦ˆë¡œ í•´ê²°) |

### Best Practices
1. **Always verify observation dimensions**: BehaviorParameters Space Size = Agent output = ONNX input
2. **Monitor TensorBoard in real-time**: Catch issues early
3. **Save checkpoints frequently**: Best model may not be final model
4. **Don't unfreeze pretrained encoders**: Use very low LR or keep frozen

---

## Next Steps (Phase F+)

| Phase | Focus | Status |
|-------|-------|--------|
| **E** | ê³¡ì„  ë„ë¡œ + ë¹„ì •í˜• ê°ë„ | âœ… **Completed (+931)** |
| **F** | Nì°¨ì„  + ì¤‘ì•™ì„  ê·œì¹™ | ğŸ”„ **Next** |
| **G** | êµì°¨ë¡œ (Tì/ì‹­ì/Yì) | ğŸ“‹ Planned |
| **H** | ì‹ í˜¸ë“± + ì •ì§€ì„  | ğŸ“‹ Planned |
| **I** | Uí„´ + íŠ¹ìˆ˜ ê¸°ë™ | ğŸ“‹ Planned |
| **J** | íš¡ë‹¨ë³´ë„ + ë³´í–‰ì | ğŸ“‹ Planned |
| **K** | ì¥ì• ë¬¼ + ê¸´ê¸‰ ìƒí™© | ğŸ“‹ Planned |
| **L** | ë³µí•© ì‹œë‚˜ë¦¬ì˜¤ í†µí•© | ğŸ“‹ Planned |

---

## Documentation

- [PRD (Product Requirements)](docs/PRD.md)
- [Training Log (Detailed)](docs/TRAINING-LOG.md)
- [Learning Roadmap](docs/LEARNING-ROADMAP.md)
- [Phase Documents](docs/phases/README.md)
- [Progress Tracking](docs/PROGRESS.md)

---

## References

- [Unity ML-Agents Documentation](https://unity-technologies.github.io/ml-agents/)
- [ML-Agents GitHub](https://github.com/Unity-Technologies/ml-agents)
- [PPO Algorithm](https://arxiv.org/abs/1707.06347)

---

**Last Updated**: 2026-01-27 | **Phase E Completed** | Phase E Reward: +931
