# Tesla FSD vs Physical Unity: Gap Analysis & Implementation Roadmap

**Document Version**: 1.0
**Created**: 2026-01-30
**Status**: Active Analysis
**Purpose**: Tesla FSD 12/13ì˜ ì•„í‚¤í…ì²˜ ë¶„ì„ ë° ë³¸ í”„ë¡œì íŠ¸ì˜ í˜„ì‹¤ì  êµ¬í˜„ ë¡œë“œë§µ ì œì‹œ

---

## 1. Executive Summary

### 1.1 ë³¸ í”„ë¡œì íŠ¸ì™€ Tesla FSDì˜ ê·¼ë³¸ì  ì°¨ì´ 3ì¤„ ìš”ì•½

1. **ë°ì´í„° ê·œëª¨**: TeslaëŠ” 400ë§ŒëŒ€ ì´ìƒ í”Œë¦¿ì˜ Shadow Mode ë°ì´í„°ë¥¼ í™œìš©, ë³¸ í”„ë¡œì íŠ¸ëŠ” Unity ì‹œë®¬ë ˆì´ì…˜ + ê³µê°œ ë°ì´í„°ì…‹(nuPlan 1282ì‹œê°„) í™œìš©
2. **ì»´í“¨íŒ… ìì›**: TeslaëŠ” Dojo Supercomputer(1.1 ExaFLOPS, D1 ì¹© ìˆ˜ì²œê°œ), ë³¸ í”„ë¡œì íŠ¸ëŠ” RTX 4090 1ëŒ€(82.6 TFLOPS FP32)
3. **ì•„í‚¤í…ì²˜ ë³µì¡ë„**: TeslaëŠ” Occupancy Network ê¸°ë°˜ 4D ê³µê°„ ì¶”ë¡  â†’ MCTS Planner, ë³¸ í”„ë¡œì íŠ¸ëŠ” Ground Truth Vector â†’ RL Policy

### 1.2 í˜„ì‹¤ì  ëª©í‘œ ì„¤ì •

**RTX 4090 1ëŒ€ë¡œ í•  ìˆ˜ ìˆëŠ” ê²ƒì˜ í•œê³„**:

| ì¸¡ë©´ | Tesla FSD | ë³¸ í”„ë¡œì íŠ¸ í˜„ì‹¤ |
|------|-----------|----------------|
| ì…ë ¥ | 8x 1280x960 Camera (Multi-view) | 242D Ground Truth Vector ë˜ëŠ” ë‹¨ì¼ 84x84 Camera |
| Perception | Occupancy Network (3D Voxel Grid) | Pre-trained ëª¨ë¸ fine-tune ë˜ëŠ” GT ì§ì ‘ ì‚¬ìš© |
| Planning | MCTS + Neural Evaluator (20 candidates) | PPO/SAC Direct Control (2D output) |
| í•™ìŠµ ë°ì´í„° | 400ë§ŒëŒ€ Fleet + Auto-Label | nuPlan 10k scenarios + Self-play |
| í•™ìŠµ ì‹œê°„ | Dojoì—ì„œ ìˆ˜ì¼ (PB ê·œëª¨ ë°ì´í„°) | RTX 4090ì—ì„œ ìˆ˜ì£¼-ìˆ˜ê°œì›” (GB-TB ê·œëª¨) |
| ëª©í‘œ | Level 2-3 ìƒìš©í™” (No geo-fence) | ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ë‚´ ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ |

**í•µì‹¬ ì°¨ì´ì **: Tesla FSDëŠ” **ìƒìš© ì œí’ˆ**, ë³¸ í”„ë¡œì íŠ¸ëŠ” **ì—°êµ¬/ê²€ì¦ í”Œë«í¼**ì´ë‹¤. ë™ì¼ ìˆ˜ì¤€ ë‹¬ì„±ì€ ë¶ˆê°€ëŠ¥í•˜ë©° ë¶ˆí•„ìš”í•˜ë‹¤.

**í•©ë¦¬ì  ëª©í‘œ**: í•™ìˆ  ì—°êµ¬ ìˆ˜ì¤€ì˜ E2E Pipeline êµ¬ì¶• + ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ + Phaseë³„ ì ì§„ì  êµ¬í˜„

---

## 2. Tesla FSD ì•„í‚¤í…ì²˜ ì „ì²´ êµ¬ì¡°

### 2.1 Vision Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TESLA FSD 12/13 VISION PIPELINE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 1: Multi-Camera Input                            â”‚    â”‚
â”‚  â”‚  8 Cameras: Front(3x), Left(2x), Right(2x), Rear(1x)               â”‚    â”‚
â”‚  â”‚  Resolution: 1280x960 @ 36 Hz                                      â”‚    â”‚
â”‚  â”‚  Total: ~88M pixels/frame                                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 2: RegNet Backbone (Multi-scale)                 â”‚    â”‚
â”‚  â”‚  Input: 8x [1280, 960, 3]                                          â”‚    â”‚
â”‚  â”‚  Backbone: RegNetY-120GF (120B FLOPS)                              â”‚    â”‚
â”‚  â”‚  Output: Multi-scale features [C1, C2, C3, C4, C5]                 â”‚    â”‚
â”‚  â”‚    C5: 1/32 resolution, 2048 channels (semantic features)          â”‚    â”‚
â”‚  â”‚    C3: 1/8 resolution, 512 channels (detail features)              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 3: BiFPN (Bi-directional FPN)                    â”‚    â”‚
â”‚  â”‚  Purpose: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì •ë³´ ìœµí•©                                      â”‚    â”‚
â”‚  â”‚  Method: Top-down + Bottom-up bidirectional feature flow           â”‚    â”‚
â”‚  â”‚  Output: Fused multi-scale features                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 4: Transformer Cross-Attention Fusion             â”‚    â”‚
â”‚  â”‚  Input: 8 camera features (ê°ê° ë‹¤ë¥¸ viewpoint)                     â”‚    â”‚
â”‚  â”‚  Method: Query-based attention (DETR-style)                        â”‚    â”‚
â”‚  â”‚    Queries: BEV grid positions (200x200x8 = 320k queries)          â”‚    â”‚
â”‚  â”‚    Keys/Values: Camera features (with camera intrinsics)           â”‚    â”‚
â”‚  â”‚  Output: Fused BEV feature map [200, 200, 256]                     â”‚    â”‚
â”‚  â”‚    - 200x200 grid = 100m x 100m @ 0.5m resolution                  â”‚    â”‚
â”‚  â”‚    - 256 channels = semantic + geometric features                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 5: Occupancy Network (3D Voxel Grid)             â”‚    â”‚
â”‚  â”‚  Input: BEV features [200, 200, 256]                               â”‚    â”‚
â”‚  â”‚  3D Decoder: Sparse 3D CNN or NeRF-style MLP                       â”‚    â”‚
â”‚  â”‚  Output: Occupancy Grid [200, 200, 16, C]                          â”‚    â”‚
â”‚  â”‚    - 16 height bins (0-8m, 0.5m steps)                             â”‚    â”‚
â”‚  â”‚    - C classes: road, vehicle, pedestrian, static, ...             â”‚    â”‚
â”‚  â”‚  Supervision: Auto-labeled 3D boxes + LiDAR pseudo-GT              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 6: Occupancy Flow (Motion Prediction)            â”‚    â”‚
â”‚  â”‚  Input: Occupancy Grid @ t-1, t, t+1 (3 frames)                    â”‚    â”‚
â”‚  â”‚  Method: 3D ConvGRU or Transformer temporal encoder                â”‚    â”‚
â”‚  â”‚  Output: Future Occupancy @ t+2, ..., t+10 (2s ahead)              â”‚    â”‚
â”‚  â”‚    - Per-voxel flow vectors [dx, dy, dz]                           â”‚    â”‚
â”‚  â”‚    - Uncertainty estimates (entropy map)                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•µì‹¬ ê¸°ìˆ **:
- **Multi-view Fusion**: 8ê°œ ì¹´ë©”ë¼ì˜ ì •ë³´ë¥¼ Transformerë¡œ BEV ê³µê°„ì— í†µí•©
- **Occupancy Network**: ì „í†µì ì¸ Object Detection ëŒ€ì‹  ë°€ì§‘ ê³µê°„ í‘œí˜„ ì‚¬ìš©
- **Temporal Modeling**: ê³¼ê±°-í˜„ì¬-ë¯¸ë˜ í”„ë ˆì„ì˜ ì‹œê°„ì  ì¼ê´€ì„± í•™ìŠµ

**ëª¨ë¸ í¬ê¸° ì¶”ì •**:
- RegNet Backbone: ~250M parameters
- Transformer Fusion: ~100M parameters
- Occupancy Decoder: ~50M parameters
- **Total**: ~400M parameters, ~120GB VRAM (FP32), ~30GB (FP16 Mixed)

### 2.2 Planning Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TESLA FSD PLANNING PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Input: Occupancy + Lane + Route                       â”‚    â”‚
â”‚  â”‚  - Occupancy Grid: [200, 200, 16] (future 2s)                      â”‚    â”‚
â”‚  â”‚  - Lane Graph: [N_lanes x waypoints x 3] (x, y, type)              â”‚    â”‚
â”‚  â”‚  - Route: [M_waypoints x 3] (GPS â†’ local map)                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 1: Monte Carlo Tree Search (Discrete)            â”‚    â”‚
â”‚  â”‚  Search Space: Discrete maneuvers                                  â”‚    â”‚
â”‚  â”‚    - Lane keep                                                     â”‚    â”‚
â”‚  â”‚    - Lane change left/right                                        â”‚    â”‚
â”‚  â”‚    - Speed up/down/maintain                                        â”‚    â”‚
â”‚  â”‚    - Merge/exit                                                    â”‚    â”‚
â”‚  â”‚  Method: MCTS with UCB1 selection                                  â”‚    â”‚
â”‚  â”‚    Simulation depth: 5-10 steps (2-4 seconds)                      â”‚    â”‚
â”‚  â”‚    Rollouts: 50-100 per decision cycle                             â”‚    â”‚
â”‚  â”‚  Output: ~20 Candidate Maneuver Sequences                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 2: Neural Network Evaluator (Continuous)         â”‚    â”‚
â”‚  â”‚  Input: 20 Candidate Sequences â†’ Continuous Trajectory             â”‚    â”‚
â”‚  â”‚  Method: Trajectory Optimizer Network                              â”‚    â”‚
â”‚  â”‚    - Input: Maneuver + Occupancy + Lane                            â”‚    â”‚
â”‚  â”‚    - Output: [N_steps x 5] (x, y, v, heading, accel)               â”‚    â”‚
â”‚  â”‚  Optimization: Gradient descent on smoothness + feasibility        â”‚    â”‚
â”‚  â”‚    - Kinematic constraints (curvature, accel limits)               â”‚    â”‚
â”‚  â”‚    - Comfort (jerk, lateral accel)                                 â”‚    â”‚
â”‚  â”‚    - Collision checking (Occupancy grid query)                     â”‚    â”‚
â”‚  â”‚  Output: 20 Optimized Trajectories                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 3: Cost Function Scoring                         â”‚    â”‚
â”‚  â”‚  Components:                                                       â”‚    â”‚
â”‚  â”‚    w_collision * C_collision(traj, occupancy)                      â”‚    â”‚
â”‚  â”‚      - SDF distance to occupied voxels                             â”‚    â”‚
â”‚  â”‚      - Penalty: exp(-dist/sigma)                                   â”‚    â”‚
â”‚  â”‚    w_comfort * C_comfort(jerk, lat_accel)                          â”‚    â”‚
â”‚  â”‚      - Jerk^2 + Lateral_accel^2 integrated                         â”‚    â”‚
â”‚  â”‚    w_intervention * C_intervention(traj)                           â”‚    â”‚
â”‚  â”‚      - Learned from driver takeover data                           â”‚    â”‚
â”‚  â”‚      - Neural network: traj â†’ P(intervention)                      â”‚    â”‚
â”‚  â”‚    w_human * C_human_likeness(traj)                                â”‚    â”‚
â”‚  â”‚      - GAN Discriminator: real driver vs model                     â”‚    â”‚
â”‚  â”‚      - Pre-trained on Fleet data                                   â”‚    â”‚
â”‚  â”‚  Total Cost: Î£ w_i * C_i(traj)                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 4: Best Trajectory Selection                     â”‚    â”‚
â”‚  â”‚  Method: argmin(cost) over 20 candidates                           â”‚    â”‚
â”‚  â”‚  Fallback: If all costs > threshold, select "safe stop"            â”‚    â”‚
â”‚  â”‚  Output: Selected Trajectory [40 steps x 5] (2s @ 20Hz)            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 5: Direct Neural Network Control                 â”‚    â”‚
â”‚  â”‚  Input: Selected Trajectory + Current State                        â”‚    â”‚
â”‚  â”‚  Method: MPC-style tracking controller (Neural Network)            â”‚    â”‚
â”‚  â”‚    - Receding horizon: 0.5s                                        â”‚    â”‚
â”‚  â”‚    - Control output: steering_angle, accel, brake                  â”‚    â”‚
â”‚  â”‚  Update Rate: 40ms (25 Hz)                                         â”‚    â”‚
â”‚  â”‚  Output: [steering, accel, brake] â†’ CAN Bus                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 6: Replanning (every 40ms)                       â”‚    â”‚
â”‚  â”‚  Condition: New Occupancy prediction available                     â”‚    â”‚
â”‚  â”‚  Action: Return to Step 1 with updated world state                 â”‚    â”‚
â”‚  â”‚  Continuity: Warm-start MCTS from previous search tree             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Planning ê³„ì‚° ë³µì¡ë„**:
- MCTS: 50 rollouts Ã— 10 steps Ã— 0.1ms = 50ms
- Neural Evaluator: 20 trajectories Ã— 2ms = 40ms
- Cost Scoring: 20 trajectories Ã— 1ms = 20ms
- **Total**: ~110ms per cycle (25Hz ë‹¬ì„± ê°€ëŠ¥)

### 2.3 Data Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TESLA FSD DATA PIPELINE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 1: Shadow Mode Data Collection                   â”‚    â”‚
â”‚  â”‚  Fleet Size: 4M+ vehicles (2024 ê¸°ì¤€)                              â”‚    â”‚
â”‚  â”‚  Trigger: FSD vs Human Driver disagreement detection               â”‚    â”‚
â”‚  â”‚    - FSD wants: lane change left                                   â”‚    â”‚
â”‚  â”‚    - Human does: lane keep                                         â”‚    â”‚
â”‚  â”‚    â†’ Record 10s clip (5s before + 5s after)                        â”‚    â”‚
â”‚  â”‚  Data Rate: ~1% of driving time (edge cases)                       â”‚    â”‚
â”‚  â”‚  Storage: ~1M clips/day â†’ ~10 PB/year                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 2: Hard Clip Selection                           â”‚    â”‚
â”‚  â”‚  Method: Criticality Score = f(TTC, Intervention, Speed)           â”‚    â”‚
â”‚  â”‚  Filter:                                                           â”‚    â”‚
â”‚  â”‚    - Intervention occurred: High priority                          â”‚    â”‚
â”‚  â”‚    - TTC < 3s: Medium priority                                     â”‚    â”‚
â”‚  â”‚    - Novel scenario (low cluster density): High priority           â”‚    â”‚
â”‚  â”‚  Selection Rate: 1% of collected clips (top 10k/day)               â”‚    â”‚
â”‚  â”‚  Diversity: K-means clustering on scenario features                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 3: Auto-Labeling (4D Reconstruction)             â”‚    â”‚
â”‚  â”‚  Input: 8 camera streams + IMU + GPS                               â”‚    â”‚
â”‚  â”‚  Method:                                                           â”‚    â”‚
â”‚  â”‚    1. SLAM: Camera pose estimation                                 â”‚    â”‚
â”‚  â”‚    2. Multi-view Stereo: Depth reconstruction                      â”‚    â”‚
â”‚  â”‚    3. 3D Object Detection: Pre-trained model                       â”‚    â”‚
â”‚  â”‚    4. 3D Tracking: Kalman Filter + Hungarian matching              â”‚    â”‚
â”‚  â”‚    5. Occupancy Grid: Voxel fusion from depth maps                 â”‚    â”‚
â”‚  â”‚  Output: Pseudo Ground Truth                                       â”‚    â”‚
â”‚  â”‚    - 3D boxes: [x, y, z, l, w, h, heading, class, track_id]        â”‚    â”‚
â”‚  â”‚    - Occupancy: [200, 200, 16] per frame                           â”‚    â”‚
â”‚  â”‚  Accuracy: ~95% (verified by human on sample)                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 4: Human Verification (Sample)                   â”‚    â”‚
â”‚  â”‚  Sample Rate: 10% of auto-labeled clips (1k/day)                   â”‚    â”‚
â”‚  â”‚  Task: Annotators verify/correct 3D boxes and occupancy            â”‚    â”‚
â”‚  â”‚  Tool: Custom 3D labeling interface (Blender-like)                 â”‚    â”‚
â”‚  â”‚  Metrics:                                                          â”‚    â”‚
â”‚  â”‚    - 3D IoU > 0.7: Accept                                          â”‚    â”‚
â”‚  â”‚    - 3D IoU < 0.5: Reject & manual label                           â”‚    â”‚
â”‚  â”‚  Feedback Loop: Auto-labeler fine-tuning quarterly                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 5: Training on Dojo                              â”‚    â”‚
â”‚  â”‚  Cluster: 1.1 ExaFLOPS (2024)                                      â”‚    â”‚
â”‚  â”‚    - D1 chips: Custom 7nm ASIC for ML training                     â”‚    â”‚
â”‚  â”‚    - Nodes: 3,000+ (each with 25 D1 chips)                         â”‚    â”‚
â”‚  â”‚  Training Config:                                                  â”‚    â”‚
â”‚  â”‚    - Batch size: 2048 (distributed across nodes)                   â”‚    â”‚
â”‚  â”‚    - Model size: 400M params (~1.6GB FP16)                         â”‚    â”‚
â”‚  â”‚    - Dataset: ~100M clips (10 PB)                                  â”‚    â”‚
â”‚  â”‚    - Duration: 3-5 days per major version                          â”‚    â”‚
â”‚  â”‚  Output: FSD Model vX.Y.Z (.onnx format for HW3)                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 6: Policy Distillation (Teacherâ†’Student)         â”‚    â”‚
â”‚  â”‚  Teacher: Large model trained on Dojo (400M params)                â”‚    â”‚
â”‚  â”‚  Student: Compact model for HW3 inference (200M params)            â”‚    â”‚
â”‚  â”‚  Method: Knowledge Distillation                                    â”‚    â”‚
â”‚  â”‚    - Soft labels from teacher (logits)                             â”‚    â”‚
â”‚  â”‚    - Mimicry loss: KL(student || teacher)                          â”‚    â”‚
â”‚  â”‚  Performance: 95-98% of teacher accuracy                           â”‚    â”‚
â”‚  â”‚  Latency: 40ms/frame (HW3 chip: 144 TOPS INT8)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Step 7: OTA Update to Fleet                           â”‚    â”‚
â”‚  â”‚  Distribution: 4M vehicles via cellular OTA                        â”‚    â”‚
â”‚  â”‚  Rollout Strategy: Gradual (0.1% â†’ 1% â†’ 10% â†’ 100%)                â”‚    â”‚
â”‚  â”‚  Monitoring: Real-time intervention metrics                        â”‚    â”‚
â”‚  â”‚    - Intervention rate increase > 20% â†’ rollback                   â”‚    â”‚
â”‚  â”‚    - Critical safety events â†’ immediate disable                    â”‚    â”‚
â”‚  â”‚  Cycle Time: 2-4 weeks per release                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê·œëª¨**:
- ì¼ì¼ ìˆ˜ì§‘: ~1M clips Ã— 10s Ã— 8 cameras Ã— 1280Ã—960 = ~100 TB/day
- ì €ì¥ ë¹„ìš©: ì—°ê°„ ~10 PB â†’ AWS S3 ê¸°ì¤€ $250k/month
- ë¼ë²¨ë§ ë¹„ìš©: ìˆ˜ë™ 10% â†’ $1M-5M/month (ì¶”ì •)

---

## 3. Gap Analysis: ì»´í¬ë„ŒíŠ¸ë³„ ë¹„êµ

### 3.1 7ê°œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë¹„êµí‘œ

| ì»´í¬ë„ŒíŠ¸ | Tesla FSD | ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬ | Gap Level | êµ¬í˜„ ë‚œì´ë„ | RTX 4090 ê°€ëŠ¥ì„± |
|----------|-----------|--------------|-----------|-----------|----------------|
| **1. Vision Perception** | HydraNet + RegNet-120GF + Occupancy Network (400M params) | Ground Truth ë²¡í„° (242D) | **CRITICAL** | Very High | ì œí•œì  (ê²½ëŸ‰ ëª¨ë¸ë§Œ) |
| **2. BEV Representation** | Transformer-based 8-camera fusion (200x200x256) | ì—†ìŒ | **CRITICAL** | High | ê°€ëŠ¥ (ë‹¨ì¼ ì¹´ë©”ë¼) |
| **3. Trajectory Prediction** | Occupancy Flow (3D ConvGRU, 2s horizon) | Constant Velocity ê°€ì • | **MAJOR** | Medium | ê°€ëŠ¥ (LSTM/GNN) |
| **4. Trajectory Planning** | MCTS + Neural Evaluator (20 candidates) | ì—†ìŒ (reactive control only) | **CRITICAL** | High | ê°€ëŠ¥ (ë‹¨ìˆœí™”) |
| **5. Route Planning** | GPS â†’ local lane-level graph | ê³ ì • Waypoint ì‹œìŠ¤í…œ | **MAJOR** | Medium | ê°€ëŠ¥ (A*) |
| **6. Vehicle Control** | Direct neural network (40ms replan) | RL Policy â†’ steering/accel | **MODERATE** | Low | ì´ë¯¸ ë‹¬ì„± |
| **7. Data Pipeline** | Shadow Mode + Auto-Label + Dojo | ML-Agents self-play | **MAJOR** | High | ë¶ˆê°€ (ê·œëª¨ ì°¨ì´) |

### 3.2 ê° Gap ìƒì„¸ ë¶„ì„

#### Gap 1: Vision Perception (CRITICAL)

**Tesla FSD**:
- 8ê°œ ì¹´ë©”ë¼ë¡œ 360ë„ ì‹œì•¼ í™•ë³´
- RegNet-120GF Backboneìœ¼ë¡œ Multi-scale feature extraction
- Transformer Cross-Attentionìœ¼ë¡œ BEV ê³µê°„ì— fusion
- Occupancy Networkë¡œ ë°€ì§‘ 3D ê³µê°„ í‘œí˜„ (200Ã—200Ã—16 voxels)
- í•™ìŠµ ë°ì´í„°: ìˆ˜ë°±ë§Œ ìë™ ë¼ë²¨ë§ëœ í´ë¦½

**ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬**:
- Unity ì‹œë®¬ë ˆì´í„°ì—ì„œ Ground Truth ë²¡í„° ì§ì ‘ ì œê³µ
- 242D observation: ego state(8D) + route(30D) + NPCs(160D) + lane(12D) + goal(12D) + history(20D)
- ì¹´ë©”ë¼ ì…ë ¥ ì—†ìŒ (ML-Agents CameraSensor ì§€ì›ì€ í•˜ì§€ë§Œ ë¯¸ì‚¬ìš©)

**Gapì˜ ë³¸ì§ˆ**:
1. **ë°ì´í„° ê·œëª¨**: TeslaëŠ” ìˆ˜ë°±ë§Œ ì‹¤ì œ ì£¼í–‰ ì´ë¯¸ì§€, ë³¸ í”„ë¡œì íŠ¸ëŠ” ì‹œë®¬ë ˆì´ì…˜
2. **ëª¨ë¸ í¬ê¸°**: 400M paramsëŠ” RTX 4090 VRAM(24GB)ì— FP16ìœ¼ë¡œ ê²¨ìš° ì ì¬ ê°€ëŠ¥, í•™ìŠµì€ ë¶ˆê°€ëŠ¥ (Gradient + Optimizer state í¬í•¨ ì‹œ ~100GB í•„ìš”)
3. **ì¶”ë¡  ì†ë„**: RegNet-120GFëŠ” ë‹¨ì¼ forward passì— ~200ms (RTX 4090 ê¸°ì¤€), FSDëŠ” HW3 ì¹©ì— ìµœì í™”ë˜ì–´ 40ms

**ê¸°ìˆ ì  ì°¨ì´**:
- Teslaì˜ Occupancy NetworkëŠ” ì „í†µì  Object Detectionì˜ í•œê³„(Occlusion, Long-tail objects) ê·¹ë³µ
- BEV representationì€ lane-level planningì— í•„ìˆ˜ì  (ì°¨ì„  ì¤‘ì‹¬ì„  í‘œí˜„, lateral offset ì •í™•ë„)
- ë³¸ í”„ë¡œì íŠ¸ì˜ GT VectorëŠ” ì •í™•í•˜ì§€ë§Œ ì‹¤ì œ ì„¼ì„œì˜ ë¶ˆí™•ì‹¤ì„± í•™ìŠµ ë¶ˆê°€

#### Gap 2: BEV Representation (CRITICAL)

**Tesla FSD**:
- Transformer Query-based fusion (DETR ìŠ¤íƒ€ì¼)
- 200Ã—200 grid @ 0.5m resolution = 100mÃ—100m ì»¤ë²„
- 256 channels: semantic (road, vehicle, pedestrian) + geometric (height, depth)

**ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬**:
- BEV í‘œí˜„ ì—†ìŒ
- Surrounding vehiclesëŠ” local coordinate (ego-centric) ë²¡í„°ë¡œ í‘œí˜„
- Lane markingë„ ego vehicle ê¸°ì¤€ left/right distanceë§Œ ì œê³µ

**Gapì˜ ë³¸ì§ˆ**:
- BEVëŠ” **spatial reasoning**ì— í•„ìˆ˜: ë³µì¡í•œ ë„ë¡œ ê¸°í•˜í•™(ê³¡ì„ , êµì°¨ë¡œ) ì´í•´
- Transformer fusionì€ ê³„ì‚° ë¹„ìš© ë†’ìŒ: 320k queries Ã— 8 cameras â†’ ~100M FLOPs
- ë‹¨ì¼ RTX 4090ìœ¼ë¡œëŠ” í•™ìŠµ ê°€ëŠ¥í•˜ë‚˜, ì‹¤ì‹œê°„ ì¶”ë¡ (< 50ms)ì€ ê²½ëŸ‰í™” í•„ìˆ˜

**í•™ìˆ  ì°¸ì¡°**:
- BEVFormer (2022): Temporal BEV representation, 50ms @ V100
- LSS (Lift-Splat-Shoot, 2020): Simpler BEV projection, 35ms @ V100
- ë³¸ í”„ë¡œì íŠ¸ëŠ” LSS ìŠ¤íƒ€ì¼ ê²½ëŸ‰ BEVê°€ í˜„ì‹¤ì 

#### Gap 3: Trajectory Prediction (MAJOR)

**Tesla FSD**:
- Occupancy Flow: ê³¼ê±° 3 frames â†’ ë¯¸ë˜ 2s (10 frames @ 5Hz)
- Per-voxel motion vectors + uncertainty
- 3D ConvGRU ë˜ëŠ” Transformer temporal model

**ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬**:
- Constant Velocity Model: `future_pos = current_pos + velocity * dt`
- NPC í–‰ë™ ì˜ˆì¸¡ ì—†ìŒ (lane change, braking ì˜ˆì¸¡ ë¶ˆê°€)

**Gapì˜ ë³¸ì§ˆ**:
- CV modelì€ ì§ì„  ì£¼í–‰ë§Œ ì •í™•, ì°¨ì„  ë³€ê²½ ì‹œ ì™„ì „íˆ ì‹¤íŒ¨
- Occupancy FlowëŠ” dense prediction â†’ ì‹ ê·œ ê°ì²´(ê°‘ìê¸° ë‚˜íƒ€ë‚œ ì°¨) ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬
- ë³¸ í”„ë¡œì íŠ¸ëŠ” Object-centric prediction (per-vehicle LSTM/GNN)ì´ í˜„ì‹¤ì 

**í•™ìˆ  ì°¸ì¡°**:
- Trajectron++ (2020): Graph-based multi-agent prediction, 15ms @ RTX 2080
- nuPlan Baseline Predictor: Simple LSTM, 5ms ì¶”ë¡ 
- ë³¸ í”„ë¡œì íŠ¸ëŠ” nuPlan Baseline ìˆ˜ì¤€ ëª©í‘œ

#### Gap 4: Trajectory Planning (CRITICAL)

**Tesla FSD**:
- 2-stage planning: MCTS (discrete maneuver) â†’ Neural Evaluator (continuous trajectory)
- 20 candidates ìƒì„± í›„ cost functionìœ¼ë¡œ ì„ íƒ
- Cost functionì— human-likeness (GAN discriminator) í¬í•¨

**ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬**:
- Direct control: RL policyê°€ [steering, accel] 2D output ì§ì ‘ ìƒì„±
- Trajectory ê°œë… ì—†ìŒ (waypoint ì—†ì´ ì¦‰ì‹œ ì œì–´ ëª…ë ¹)
- Human-likeness í•™ìŠµ ì—†ìŒ (reward functionë§Œ)

**Gapì˜ ë³¸ì§ˆ**:
- Teslaì˜ 2-stageëŠ” **interpretability** + **safety** ì¥ì : ê¶¤ì ì„ ë¨¼ì € ìƒì„±í•˜ë¯€ë¡œ ê²€ì¦ ê°€ëŠ¥
- Direct controlì€ black-box, ì™œ ê·¸ steeringì„ ì„ íƒí–ˆëŠ”ì§€ ì„¤ëª… ë¶ˆê°€
- MCTSëŠ” ê³„ì‚° ë¹„ìš© ë†’ì§€ë§Œ exploration ìš°ìˆ˜
- RL direct controlì€ ë¹ ë¥´ì§€ë§Œ sample efficiency ë‚®ìŒ

**êµ¬í˜„ ê°€ëŠ¥ì„±**:
- Sampling-based planner (CEM: Cross-Entropy Method) ì‚¬ìš© ê°€ëŠ¥
- ë³¸ í”„ë¡œì íŠ¸ì˜ RLì„ trajectory outputìœ¼ë¡œ ë³€ê²½ â†’ PID/MPC low-level controller ì¶”ê°€
- ê³„ì‚° ë³µì¡ë„: CEM 100 samples Ã— 20 steps â†’ ~5ms (í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ ë³‘ë ¬í™” ê°€ëŠ¥)

#### Gap 5: Route Planning (MAJOR)

**Tesla FSD**:
- GPS ê²½ë¡œ â†’ HD Map â†’ Lane-level graph
- A* on lane graph with dynamic cost (traffic, road closure)
- Rerouting every 10s or on significant deviation

**ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬**:
- ê³ ì • waypoints (Unity Inspectorì—ì„œ ìˆ˜ë™ ë°°ì¹˜)
- ë™ì  ê²½ë¡œ ë³€ê²½ ì—†ìŒ
- Lane-level routing ì—†ìŒ (ë‹¨ìˆœ position target)

**Gapì˜ ë³¸ì§ˆ**:
- ê³ ì • waypointëŠ” ë‹¨ìˆœ íŠ¸ë™ í•™ìŠµì—ë§Œ ìœ íš¨
- ì‹¤ì œ í™˜ê²½: êµì°¨ë¡œì—ì„œ ì¢Œ/ìš°íšŒì „ ì„ íƒ, ìš°íšŒë¡œ íƒìƒ‰ í•„ìš”
- Lane graph êµ¬ì¡° ì—†ìŒ â†’ lane change decision ë¶ˆê°€ëŠ¥

**êµ¬í˜„ ê°€ëŠ¥ì„±**:
- Unity ë‚´ Road graph ìƒì„± (nodes + edges)
- A* pathfinding êµ¬í˜„ (C# ë˜ëŠ” Python)
- Lane connectivity ì •ì˜ (left/right adjacent lanes)
- ë³µì¡ë„: O(E log V), ì‹¤ì‹œê°„ ê°€ëŠ¥ (ìˆ˜ë°± ë…¸ë“œ ê¸°ì¤€ < 1ms)

#### Gap 6: Vehicle Control (MODERATE)

**Tesla FSD**:
- Direct Neural Network: trajectory â†’ [steering, accel, brake]
- MPC-style receding horizon (0.5s)
- 40ms update rate (25Hz)

**ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬**:
- RL Policy: observation â†’ [steering, accel]
- Update rate: 50ms (20Hz, Unity FixedUpdate)
- Unity ë¬¼ë¦¬ ì—”ì§„ í†µí•© (Rigidbody)

**Gapì˜ ë³¸ì§ˆ**:
- ì´ë¯¸ ìœ ì‚¬í•œ ìˆ˜ì¤€ ë‹¬ì„±
- ì°¨ì´ì : TeslaëŠ” trajectory tracking, ë³¸ í”„ë¡œì íŠ¸ëŠ” direct control
- LatencyëŠ” 50msë¡œ ì¶©ë¶„ (human reaction time 200-300ms)

**ê°œì„  ë°©í–¥**:
- Trajectory output ì¶”ê°€ ì‹œ PID/MPC í•„ìš”
- í˜„ì¬ êµ¬ì¡°ë¡œëŠ” í° ë¬¸ì œ ì—†ìŒ

#### Gap 7: Data Pipeline (MAJOR)

**Tesla FSD**:
- 400ë§ŒëŒ€ í”Œë¦¿ â†’ ì¼ì¼ 100TB ìˆ˜ì§‘
- Shadow Mode: FSD vs Human disagreement ìë™ ê°ì§€
- Auto-labeling: 4D reconstruction + 3D tracking
- Dojo Supercomputer: 1.1 ExaFLOPS

**ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬**:
- Unity ì‹œë®¬ë ˆì´ì…˜ self-play
- ML-Agents ë³‘ë ¬ í™˜ê²½ (16 areas)
- RTX 4090 ë‹¨ì¼ GPU: ~10M steps/day

**Gapì˜ ë³¸ì§ˆ**:
- **ê·œëª¨ ì°¨ì´**: 10,000ë°° ì´ìƒ (100TB vs 10GB)
- **ë°ì´í„° í’ˆì§ˆ**: ì‹¤ì œ ì£¼í–‰ vs ì‹œë®¬ë ˆì´ì…˜ (Sim-to-Real gap)
- **ì»´í“¨íŒ…**: Dojo(1.1 ExaFLOPS) vs RTX 4090(0.082 TFLOPS FP32) = 13,000ë°°

**ì™„ì „íˆ ê·¹ë³µ ë¶ˆê°€ëŠ¥í•œ Gap**:
- Shadow Mode fleet ë°ì´í„°ëŠ” ëŒ€ì²´ ë¶ˆê°€
- ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ë³´ì™„ ê°€ëŠ¥í•˜ì§€ë§Œ generalization í•œê³„ ì¡´ì¬
- ë³¸ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ” **ì•Œê³ ë¦¬ì¦˜ ê²€ì¦**ì´ë¯€ë¡œ, ê·œëª¨ëŠ” ë¶ˆí•„ìš”

---

## 4. í˜„ì‹¤ì  êµ¬í˜„ ë¡œë“œë§µ (RTX 4090 ë‹¨ì¼ GPU)

### 4.1 í•˜ë“œì›¨ì–´ ì œì•½ ë¶„ì„

**RTX 4090 ìŠ¤í™**:
- CUDA Cores: 16,384
- Tensor Cores: 512 (4ì„¸ëŒ€)
- VRAM: 24GB GDDR6X
- Memory Bandwidth: 1,008 GB/s
- FP32 Performance: 82.6 TFLOPS
- FP16 Performance: 165.2 TFLOPS (Tensor Core)
- INT8 Performance: 330.3 TOPS (Tensor Core)
- TDP: 450W

**í•™ìŠµ ì‹œ VRAM ì‚¬ìš©ëŸ‰ ì¶”ì •**:
```
Model Parameters (FP32):     P * 4 bytes
Gradients (FP32):            P * 4 bytes
Optimizer State (Adam):      P * 8 bytes (2 moments)
Activations (batch=B):       ~P * B * 2 bytes (ì¶”ì •)

Total (FP32): P * (4 + 4 + 8 + 2B) = P * (16 + 2B)

Example: P=100M, B=32
  â†’ 100M * (16 + 64) = 8GB

Maximum model size @ VRAM=24GB, B=32:
  P_max = 24GB / 80 bytes â‰ˆ 300M params (FP32)
  P_max = 24GB / 44 bytes â‰ˆ 545M params (FP16 Mixed Precision)
```

**ì¶”ë¡  ì‹œ ì œì•½**:
- Unity Sentis ì‚¬ìš© ì‹œ: ~2GB VRAM ì˜ˆì•½ (Unity ìì²´)
- ì‹¤ì‹œê°„ ì¶”ë¡ (< 50ms) ëª©í‘œ: ëª¨ë¸ í¬ê¸° < 50M params

**ë‹¨ì¼ GPU í•™ìŠµ ì†ë„ ì œì•½**:
```
Throughput (samples/sec) = GPU_FLOPS / (FLOPs_per_sample)

Example: RegNet-120GF
  FLOPs_per_sample = 120B
  GPU_FLOPS = 165 TFLOPS (FP16)
  Throughput = 165T / 120B = 1,375 samples/sec

Batch=32 â†’ 43 batches/sec
PPO epoch (10 minibatches, 2048 samples) â†’ 1.5 sec/epoch

í˜„ì‹¤: Data loading, CPU overhead ê³ ë ¤ â†’ ì‹¤ì œ 0.3-0.5x
  â†’ PPO update 3-5 sec/epoch
```

### 4.2 êµ¬í˜„ ê°€ëŠ¥í•œ ê²ƒ vs ë¶ˆê°€ëŠ¥í•œ ê²ƒ

| ì»´í¬ë„ŒíŠ¸ | êµ¬í˜„ ê°€ëŠ¥ ì—¬ë¶€ | ëŒ€ì•ˆ / ì´ìœ  | VRAM | í•™ìŠµ ì‹œê°„ |
|----------|------------|-----------|------|---------|
| **Camera-based Perception** | ì œí•œì  ê°€ëŠ¥ | ML-Agents CameraSensor 84x84 (small CNN) | 2-4GB | ìˆ˜ì¼-ìˆ˜ì£¼ |
| **Multi-camera BEV** | ë¶ˆê°€ | ë‹¨ì¼ ì¹´ë©”ë¼ BEVë¡œ ì¶•ì†Œ (LSS ë°©ì‹) | 8-12GB | ìˆ˜ì£¼ |
| **Occupancy Network** | ë¶ˆê°€ (í•™ìŠµ ë°ì´í„° ë¶€ì¡±) | GT-based occupancy grid from Unity | - | - |
| **Trajectory Prediction** | ê°€ëŠ¥ | LSTM/GNN ê²½ëŸ‰ ëª¨ë¸ (nuPlan baseline ìˆ˜ì¤€) | 1-2GB | ìˆ˜ì¼ |
| **MCTS Planning** | ë¶ˆê°€ (ê³„ì‚° ë¹„ìš©) | CEM (Cross-Entropy Method) sampling-based | - | - |
| **Neural Planner** | ê°€ëŠ¥ | RL policy output = trajectory waypoints | 3-5GB | ìˆ˜ì£¼ |
| **Route Planning** | ê°€ëŠ¥ | A* on Unity road graph | - | ìˆ˜ì‹œê°„ (êµ¬í˜„) |
| **GAIL/IL** | ê°€ëŠ¥ | ML-Agents ë‚´ì¥ ê¸°ëŠ¥ ì‚¬ìš© | 4-6GB | ìˆ˜ì¼ |
| **World Model** | ì œí•œì  ê°€ëŠ¥ | Dreamer-v3 ì†Œê·œëª¨ (2D simplified) | 6-10GB | ìˆ˜ì£¼ |
| **Shadow Mode** | ë¶ˆê°€ | ì‹œë®¬ë ˆì´ì…˜ self-playë¡œ ëŒ€ì²´ | - | - |
| **400M Param Model** | ë¶ˆê°€ (í•™ìŠµ ë¶ˆê°€) | 50-100M param ëª¨ë¸ë¡œ ì¶•ì†Œ | 20-24GB | ìˆ˜ê°œì›” |

**ê²°ë¡ **:
- **ê°€ëŠ¥**: Prediction (LSTM), Planning (CEM/RL), Route (A*), IL (GAIL)
- **ì œí•œì **: BEV (ë‹¨ì¼ ì¹´ë©”ë¼), World Model (2D)
- **ë¶ˆê°€ëŠ¥**: Multi-camera Occupancy, MCTS, Fleet data

### 4.3 ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš

Phase 5 ë‚´ë¶€ì—ì„œì˜ í™•ì¥ (Stages 0-L):

```
í˜„ì¬ ìƒíƒœ (2026-01-30):
  Stage 0: Foundation (Lane Keeping)              [âœ… ì™„ë£Œ]
  Stage A: Dense Overtaking                       [âœ… ì™„ë£Œ]
  Stage B v2: Decision Making                     [âœ… ì™„ë£Œ]
  Stage C: Multi-NPC Generalization               [ğŸ“‹ ì„¤ê³„ ì™„ë£Œ]
  Stage D v2: Lane Observation (254D)             [ğŸ”„ ì§„í–‰ì¤‘]
  Stage E-G: Curved Roads, Multi-lane, Intersection [ğŸ“‹ ê³„íš]

í™•ì¥ ê³„íš (Tesla FSD ê¸°ëŠ¥ ì¶”ê°€):
  Stage 5A: Reactive RL Control (2D output)       [âœ… í˜„ì¬]
  Stage 5B: Trajectory Output (N waypoints)       [ğŸ“‹ ê³„íš]
    - Action space ë³€ê²½: 2D â†’ [N x 2] waypoints
    - Rewardì— trajectory smoothness ì¶”ê°€
    - ì˜ˆìƒ: 2-4M steps, 1-2ì£¼

  Stage 5C: Prediction Integration (LSTM)         [ğŸ“‹ ê³„íš]
    - NPC trajectory predictor í•™ìŠµ (offline)
    - Predicted trajectories â†’ observation ì¶”ê°€
    - ì˜ˆìƒ: 1M steps, 3-5ì¼

  Stage 5D: Camera Perception (Single camera)     [ğŸ“‹ ê³„íš]
    - ML-Agents CameraSensor ì¶”ê°€ (84x84)
    - CNN encoder (nature_cnn) í•™ìŠµ
    - Vector obs + Image obs fusion
    - ì˜ˆìƒ: 5-10M steps, 2-4ì£¼

  Stage 5E: BEV Representation (LSS-style)        [ğŸ“‹ ê³„íš]
    - ë‹¨ì¼ ì¹´ë©”ë¼ â†’ BEV grid (50x50)
    - Lift-Splat-Shoot ë°©ì‹ êµ¬í˜„
    - BEV features â†’ RL policy
    - ì˜ˆìƒ: 10-15M steps, 4-8ì£¼

  Stage 5F: Full E2E Pipeline (Camera â†’ Trajectory) [ğŸ“‹ ê³„íš]
    - Camera â†’ BEV â†’ Prediction â†’ Planning
    - End-to-end gradient flow
    - nuPlan benchmark í…ŒìŠ¤íŠ¸
    - ì˜ˆìƒ: 20-30M steps, 8-12ì£¼
```

**ìš°ì„ ìˆœìœ„ (í˜„ì‹¤ì  ìˆœì„œ)**:
1. **Stage 5B: Trajectory Output** (ê°€ì¥ ì¤‘ìš”, ì¦‰ì‹œ êµ¬í˜„ ê°€ëŠ¥)
2. **Stage 5C: Prediction Integration** (Constant Velocity ëŒ€ì²´)
3. **Route Planning ì¶”ê°€** (WaypointManager í™•ì¥)
4. **Stage 5D: Camera Perception** (ì‹¤í—˜ì )
5. **Stage 5E: BEV** (ì¥ê¸° ì—°êµ¬)

### 4.4 TECH-SPEC.md ë§¤í•‘ ë° í˜„ì‹¤ì  ìˆ˜ì •

**TECH-SPEC.mdì—ì„œ ì •ì˜ë˜ì—ˆì§€ë§Œ ë¯¸êµ¬í˜„**:

| TECH-SPEC ì»´í¬ë„ŒíŠ¸ | ì •ì˜ ìœ„ì¹˜ | êµ¬í˜„ ìƒíƒœ | Tesla ëŒ€ì‘ | í˜„ì‹¤ì  êµ¬í˜„ ë°©ë²• |
|------------------|---------|---------|-----------|--------------|
| **BEVEncoder** | 3.4 Modular Encoder | ë¯¸êµ¬í˜„ | RegNet + BiFPN | LSS (Lift-Splat-Shoot) ê²½ëŸ‰ ë²„ì „, ë‹¨ì¼ ì¹´ë©”ë¼ |
| **TrajectoryPredictor** | 3.5 (ì–¸ê¸‰ë§Œ) | ë¯¸êµ¬í˜„ | Occupancy Flow | nuPlan Baseline LSTM (5 agents Ã— 2s) |
| **ObservationEncoder Level 2-4** | 3.3.1 | Level 1ë§Œ êµ¬í˜„ | Multi-modal Fusion | Level 2: CNN ì¶”ê°€, Level 3: BEV ì¶”ê°€, Level 4: Temporal LSTM |
| **GAIL Discriminator** | 3.6, 4.2 | ì„¤ì •ë§Œ ì¡´ì¬ | Human-likeness GAN | ML-Agents GAIL êµ¬í˜„ í™œìš©, nuPlan expert demo í•„ìš” |
| **Trajectory Output** | ì—†ìŒ | ì—†ìŒ (2D control) | MCTS + Neural Evaluator | Action space â†’ [N waypoints Ã— 2], N=10 (2s @ 5Hz) |

**í˜„ì‹¤ì  ìˆ˜ì • ì œì•ˆ**:

```python
# python/src/models/planning/encoder.py (ìˆ˜ì •ì•ˆ)

class ObservationEncoder(nn.Module):
    """
    Level 1: Vector only (í˜„ì¬)
    Level 2: Vector + CNN (Camera 84x84)
    Level 3: Vector + BEV (ë‹¨ì¼ ì¹´ë©”ë¼ LSS)
    Level 4: Vector + BEV + Temporal (LSTM)
    """
    def __init__(self, level=1, ...):
        super().__init__()
        self.level = level

        # Level 1: Vector encoders (ê¸°ì¡´)
        self.ego_encoder = nn.Sequential(...)
        self.route_encoder = nn.Sequential(...)
        self.surr_encoder = nn.Sequential(...)

        if level >= 2:
            # Level 2: Camera encoder
            self.camera_encoder = NatureCNN(
                input_shape=(84, 84, 3),
                output_dim=128
            )

        if level >= 3:
            # Level 3: BEV encoder (LSS-style)
            self.bev_encoder = LSS_BEV(
                camera_features_dim=128,
                bev_grid_size=(50, 50),
                bev_feature_dim=64
            )

        if level >= 4:
            # Level 4: Temporal encoder
            self.temporal_encoder = nn.LSTM(
                input_size=encoded_dim,
                hidden_size=256,
                num_layers=2
            )
```

**VRAM ìš”êµ¬ëŸ‰**:
- Level 1 (í˜„ì¬): ~3GB
- Level 2 (+CNN): ~4GB
- Level 3 (+BEV): ~8GB
- Level 4 (+LSTM): ~10GB

---

## 5. í•µì‹¬ ê¸°ìˆ  êµ¬í˜„ ìƒì„¸

### 5.1 Trajectory Planning ì¶”ê°€ (ê°€ì¥ ì¤‘ìš”í•œ Gap)

**í˜„ì¬**: RL â†’ [steering, acceleration] (2D output)
**ëª©í‘œ**: RL â†’ [trajectory waypoints] â†’ PID/MPC â†’ [steering, acceleration]

#### 5.1.1 Action Space ë³€ê²½

```python
# python/src/models/planning/policy.py (ìˆ˜ì •)

class TrajectoryPlanningPolicy(nn.Module):
    """
    Output: Trajectory waypoints instead of direct control
    """
    def __init__(self, obs_dim=256, num_waypoints=10, hidden_dim=256):
        super().__init__()

        # Encoder (ê¸°ì¡´ê³¼ ë™ì¼)
        self.encoder = ObservationEncoder()

        # Actor: Trajectory decoder
        self.trajectory_decoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_waypoints * 2)  # N waypoints Ã— (x, y)
        )

        # ë˜ëŠ” RNN decoder (ë” smooth)
        self.rnn_decoder = nn.GRU(
            input_size=obs_dim,
            hidden_size=hidden_dim,
            num_layers=2
        )
        self.waypoint_head = nn.Linear(hidden_dim, 2)

    def forward(self, obs):
        encoded = self.encoder(*obs)

        # Option 1: MLP decoder (parallel generation)
        trajectory_flat = self.trajectory_decoder(encoded)
        trajectory = trajectory_flat.view(-1, self.num_waypoints, 2)

        # Option 2: RNN decoder (sequential, auto-regressive)
        hidden = encoded.unsqueeze(0).repeat(2, 1, 1)  # 2 layers
        waypoints = []
        for t in range(self.num_waypoints):
            out, hidden = self.rnn_decoder(encoded.unsqueeze(0), hidden)
            wp = self.waypoint_head(out.squeeze(0))
            waypoints.append(wp)
        trajectory = torch.stack(waypoints, dim=1)

        return trajectory
```

#### 5.1.2 Low-level Controller (PID/MPC)

```csharp
// Assets/Scripts/Controllers/TrajectoryTracker.cs (ì‹ ê·œ)

using UnityEngine;

public class TrajectoryTracker : MonoBehaviour
{
    // PID gains for steering
    public float Kp_steer = 1.5f;
    public float Ki_steer = 0.01f;
    public float Kd_steer = 0.3f;

    // PID gains for speed
    public float Kp_speed = 2.0f;
    public float Ki_speed = 0.05f;
    public float Kd_speed = 0.5f;

    private float integral_steer = 0f;
    private float prev_error_steer = 0f;
    private float integral_speed = 0f;
    private float prev_error_speed = 0f;

    public (float steering, float accel) TrackTrajectory(
        Vector2[] trajectory,  // N waypoints in local coordinates
        Vector3 currentPos,
        Quaternion currentRot,
        float currentSpeed,
        float dt = 0.02f
    )
    {
        // 1. Find closest waypoint on trajectory (lookahead)
        int lookahead_index = Mathf.Min(5, trajectory.Length - 1);  // 0.5s ahead
        Vector2 target_local = trajectory[lookahead_index];

        // 2. Steering control (Pure Pursuit variant)
        float crosstrack_error = target_local.x;  // lateral offset
        float heading_error = Mathf.Atan2(target_local.y, target_local.x);

        // PID for steering
        integral_steer += crosstrack_error * dt;
        float derivative_steer = (crosstrack_error - prev_error_steer) / dt;
        float steering = Kp_steer * crosstrack_error
                       + Ki_steer * integral_steer
                       + Kd_steer * derivative_steer;
        steering = Mathf.Clamp(steering, -0.5f, 0.5f);  // rad

        prev_error_steer = crosstrack_error;

        // 3. Speed control
        float target_speed = CalculateTargetSpeed(trajectory, lookahead_index);
        float speed_error = target_speed - currentSpeed;

        integral_speed += speed_error * dt;
        float derivative_speed = (speed_error - prev_error_speed) / dt;
        float accel = Kp_speed * speed_error
                    + Ki_speed * integral_speed
                    + Kd_speed * derivative_speed;
        accel = Mathf.Clamp(accel, -4.0f, 2.0f);  // m/s^2

        prev_error_speed = speed_error;

        return (steering, accel);
    }

    private float CalculateTargetSpeed(Vector2[] trajectory, int lookahead_index)
    {
        // Calculate curvature from trajectory
        if (lookahead_index < 2) return 15f;  // default

        Vector2 p0 = trajectory[lookahead_index - 2];
        Vector2 p1 = trajectory[lookahead_index - 1];
        Vector2 p2 = trajectory[lookahead_index];

        // Menger curvature: k = 4 * Area(triangle) / (|p0-p1| * |p1-p2| * |p2-p0|)
        float area = Mathf.Abs((p1.x - p0.x) * (p2.y - p0.y) - (p2.x - p0.x) * (p1.y - p0.y)) / 2f;
        float d01 = Vector2.Distance(p0, p1);
        float d12 = Vector2.Distance(p1, p2);
        float d20 = Vector2.Distance(p2, p0);
        float curvature = 4f * area / (d01 * d12 * d20 + 1e-6f);

        // Target speed based on curvature (v = sqrt(a_lat_max / k))
        float max_lat_accel = 3.0f;  // m/s^2
        float target_speed = Mathf.Sqrt(max_lat_accel / (curvature + 1e-6f));
        return Mathf.Clamp(target_speed, 5f, 20f);
    }
}
```

#### 5.1.3 Reward ìˆ˜ì •

```python
# python/src/models/planning/reward.py (ìˆ˜ì •)

class TrajectoryReward:
    def __init__(self, config):
        self.weights = config.get('reward_weights', {
            'progress': 1.0,
            'trajectory_smoothness': 0.5,     # NEW
            'trajectory_feasibility': 0.3,    # NEW
            'tracking_error': -0.2,           # NEW
            'collision': -5.0,
            # ... (ê¸°ì¡´ í•­ëª©)
        })

    def compute(self, state, action_trajectory, next_state, info, dt=0.02):
        reward = 0.0

        # 1. Trajectory smoothness (jerk along trajectory)
        trajectory = action_trajectory  # [N, 2]
        if len(trajectory) >= 3:
            # Numerical 2nd derivative
            accel = (trajectory[2:] - 2*trajectory[1:-1] + trajectory[:-2]) / (dt**2)
            jerk = torch.mean(torch.abs(accel))
            reward += self.weights['trajectory_smoothness'] * torch.exp(-jerk / 5.0)

        # 2. Trajectory feasibility (within kinematic constraints)
        max_curvature = 0.3  # 1/R, R_min ~ 3m
        for i in range(len(trajectory) - 2):
            p0, p1, p2 = trajectory[i], trajectory[i+1], trajectory[i+2]
            # Menger curvature
            area = torch.abs((p1[0]-p0[0])*(p2[1]-p0[1]) - (p2[0]-p0[0])*(p1[1]-p0[1])) / 2
            d01 = torch.norm(p1 - p0)
            d12 = torch.norm(p2 - p1)
            d20 = torch.norm(p0 - p2)
            k = 4 * area / (d01 * d12 * d20 + 1e-6)

            if k > max_curvature:
                reward += self.weights['trajectory_feasibility'] * (max_curvature - k)

        # 3. Tracking error (actual vs planned)
        if 'planned_waypoint' in info and 'actual_position' in info:
            tracking_error = torch.norm(info['actual_position'] - info['planned_waypoint'])
            reward += self.weights['tracking_error'] * tracking_error

        # ... (ê¸°ì¡´ reward í•­ëª©)

        return reward, done
```

#### 5.1.4 í•™ìŠµ íŒŒë¼ë¯¸í„°

```yaml
# python/configs/planning/trajectory_ppo.yaml (ì‹ ê·œ)

algorithm: PPO
environment:
  name: ADPlanningTrajectory-v0
  num_envs: 8
  max_episode_steps: 1000

  action_space:
    type: continuous
    shape: [10, 2]  # 10 waypoints Ã— (x, y)
    low: [-5.0, 0.0]  # local coordinates
    high: [5.0, 20.0]

network:
  encoder:
    level: 1  # Vector only initially
    ego_dim: 8
    route_dim: 30
    surr_dim: 40
    hidden_dim: 256

  trajectory_decoder:
    type: rnn  # 'mlp' or 'rnn'
    num_waypoints: 10
    hidden_dim: 256
    num_layers: 2

ppo:
  learning_rate: 3e-4
  batch_size: 2048
  minibatch_size: 128  # larger for trajectory output
  epochs_per_update: 10
  clip_ratio: 0.2

training:
  total_steps: 5_000_000
  eval_interval: 50_000
  checkpoint_interval: 100_000

  # Curriculum (optional)
  curriculum:
    num_waypoints:
      start: 5
      end: 10
      steps: 1_000_000
```

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 5M steps @ RTX 4090
- Throughput: ~5,000 steps/sec (action space ì¦ê°€ë¡œ ì†ë„ ê°ì†Œ)
- Total: 5M / 5k = 1,000ì´ˆ = ~16ë¶„ (ë‚™ê´€ì )
- í˜„ì‹¤: overhead ê³ ë ¤ â†’ 2-4ì‹œê°„

### 5.2 Prediction Module ì¶”ê°€

**í˜„ì¬**: Constant Velocity
**ëª©í‘œ**: LSTM-based trajectory prediction (nuPlan baseline ìˆ˜ì¤€)

#### 5.2.1 ëª¨ë¸ ì•„í‚¤í…ì²˜

```python
# python/src/models/prediction/lstm_predictor.py (ì‹ ê·œ)

import torch
import torch.nn as nn

class LSTMTrajectoryPredictor(nn.Module):
    """
    Predict future trajectories of surrounding agents

    Input: Past trajectories (T_past frames)
    Output: Future trajectories (T_future frames)
    """
    def __init__(
        self,
        input_dim=5,        # [x, y, vx, vy, heading]
        hidden_dim=128,
        num_layers=2,
        T_past=10,          # 1s @ 10Hz
        T_future=20,        # 2s @ 10Hz
        max_agents=8
    ):
        super().__init__()
        self.T_past = T_past
        self.T_future = T_future
        self.max_agents = max_agents

        # Per-agent encoder
        self.agent_encoder = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        # Social context (optional, for multi-agent interaction)
        self.social_encoder = nn.Sequential(
            nn.Linear(hidden_dim * max_agents, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Decoder (future trajectory)
        self.decoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        self.output_head = nn.Linear(hidden_dim, 2)  # (x, y) per timestep

    def forward(self, past_trajectories, agent_mask=None):
        """
        Args:
            past_trajectories: [batch, max_agents, T_past, input_dim]
            agent_mask: [batch, max_agents] (1 if agent exists, 0 otherwise)

        Returns:
            future_trajectories: [batch, max_agents, T_future, 2]
        """
        batch_size = past_trajectories.shape[0]

        # Encode past trajectories for each agent
        agent_features = []
        for i in range(self.max_agents):
            agent_past = past_trajectories[:, i, :, :]  # [batch, T_past, input_dim]
            _, (hidden, cell) = self.agent_encoder(agent_past)
            agent_features.append(hidden[-1])  # [batch, hidden_dim]

        agent_features = torch.stack(agent_features, dim=1)  # [batch, max_agents, hidden_dim]

        # Social context (global interaction)
        if agent_mask is not None:
            agent_features = agent_features * agent_mask.unsqueeze(-1)
        social_context = self.social_encoder(agent_features.flatten(1))  # [batch, hidden_dim]

        # Decode future trajectories
        future_trajectories = []
        decoder_input = social_context.unsqueeze(1)  # [batch, 1, hidden_dim]
        hidden_state = None

        for t in range(self.T_future):
            out, hidden_state = self.decoder(decoder_input, hidden_state)
            waypoint = self.output_head(out)  # [batch, 1, 2]
            future_trajectories.append(waypoint)
            decoder_input = out  # Auto-regressive

        future_trajectories = torch.cat(future_trajectories, dim=1)  # [batch, T_future, 2]

        # Broadcast to all agents (simplified, ì‹¤ì œë¡œëŠ” per-agent prediction)
        future_trajectories = future_trajectories.unsqueeze(1).repeat(1, self.max_agents, 1, 1)

        return future_trajectories
```

#### 5.2.2 ë°ì´í„° ìˆ˜ì§‘ (Unity)

```csharp
// Assets/Scripts/Data/TrajectoryRecorder.cs (ì‹ ê·œ)

using System.Collections.Generic;
using UnityEngine;
using System.IO;

public class TrajectoryRecorder : MonoBehaviour
{
    public List<GameObject> npcs;
    public float recordRate = 10f;  // 10 Hz
    public int maxFrames = 30;      // 3s total (1s past + 2s future)

    private List<List<Vector3>> trajectories;
    private float timer = 0f;
    private string outputPath = "datasets/trajectories/";

    void Start()
    {
        trajectories = new List<List<Vector3>>();
        foreach (var npc in npcs)
        {
            trajectories.Add(new List<Vector3>());
        }

        Directory.CreateDirectory(outputPath);
    }

    void Update()
    {
        timer += Time.deltaTime;

        if (timer >= 1f / recordRate)
        {
            for (int i = 0; i < npcs.Count; i++)
            {
                if (npcs[i] != null)
                {
                    trajectories[i].Add(npcs[i].transform.position);

                    if (trajectories[i].Count > maxFrames)
                    {
                        trajectories[i].RemoveAt(0);
                    }
                }
            }
            timer = 0f;
        }
    }

    public void SaveTrajectories(string filename)
    {
        using (StreamWriter writer = new StreamWriter(outputPath + filename))
        {
            writer.WriteLine("agent_id,frame,x,y,z,vx,vy,heading");

            for (int i = 0; i < trajectories.Count; i++)
            {
                for (int t = 0; t < trajectories[i].Count; t++)
                {
                    Vector3 pos = trajectories[i][t];
                    Vector3 vel = (t > 0) ? (trajectories[i][t] - trajectories[i][t-1]) * recordRate : Vector3.zero;
                    float heading = Mathf.Atan2(vel.z, vel.x);

                    writer.WriteLine($"{i},{t},{pos.x},{pos.y},{pos.z},{vel.x},{vel.z},{heading}");
                }
            }
        }
    }
}
```

#### 5.2.3 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

```python
# python/src/training/train_prediction.py (ì‹ ê·œ)

import torch
from torch.utils.data import Dataset, DataLoader
from models.prediction.lstm_predictor import LSTMTrajectoryPredictor
import pandas as pd

class TrajectoryDataset(Dataset):
    def __init__(self, csv_path, T_past=10, T_future=20):
        self.data = pd.read_csv(csv_path)
        self.T_past = T_past
        self.T_future = T_future

        # Group by agent_id and extract sequences
        self.sequences = []
        for agent_id in self.data['agent_id'].unique():
            agent_data = self.data[self.data['agent_id'] == agent_id].sort_values('frame')

            for i in range(len(agent_data) - T_past - T_future):
                past = agent_data.iloc[i:i+T_past][['x', 'y', 'vx', 'vy', 'heading']].values
                future = agent_data.iloc[i+T_past:i+T_past+T_future][['x', 'y']].values
                self.sequences.append((past, future))

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        past, future = self.sequences[idx]
        return torch.FloatTensor(past), torch.FloatTensor(future)

# Training loop
model = LSTMTrajectoryPredictor()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = torch.nn.MSELoss()

dataset = TrajectoryDataset('datasets/trajectories/train.csv')
dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

for epoch in range(50):
    for past, future_gt in dataloader:
        past = past.unsqueeze(1)  # [batch, 1 agent, T_past, 5]

        future_pred = model(past)[:, 0, :, :]  # [batch, T_future, 2]

        loss = criterion(future_pred, future_gt)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch}: Loss {loss.item():.4f}")

torch.save(model.state_dict(), 'models/prediction/lstm_predictor.pth')
```

**í•™ìŠµ ì‹œê°„**: RTX 4090
- ë°ì´í„°ì…‹: 10k sequences (nuPlan mini subset)
- Batch size: 128
- Epochs: 50
- ì˜ˆìƒ: ~30ë¶„

#### 5.2.4 Unity í†µí•©

```csharp
// Assets/Scripts/Inference/PredictionInference.cs (ì‹ ê·œ)

using Unity.Sentis;
using UnityEngine;

public class PredictionInference : MonoBehaviour
{
    public ModelAsset predictionModel;
    private Worker worker;

    void Start()
    {
        var runtimeModel = ModelLoader.Load(predictionModel);
        worker = new Worker(runtimeModel, BackendType.GPUCompute);
    }

    public Vector2[] PredictTrajectory(
        Vector3[] pastPositions,  // 10 frames
        Vector3[] pastVelocities
    )
    {
        // Prepare input tensor [1, 1, 10, 5]
        float[] inputData = new float[1 * 1 * 10 * 5];
        for (int i = 0; i < 10; i++)
        {
            inputData[i*5 + 0] = pastPositions[i].x;
            inputData[i*5 + 1] = pastPositions[i].z;
            inputData[i*5 + 2] = pastVelocities[i].x;
            inputData[i*5 + 3] = pastVelocities[i].z;
            inputData[i*5 + 4] = Mathf.Atan2(pastVelocities[i].z, pastVelocities[i].x);
        }

        using var inputTensor = new Tensor<float>(new TensorShape(1, 1, 10, 5), inputData);
        worker.SetInput("past_trajectories", inputTensor);
        worker.Schedule();

        var output = worker.PeekOutput("future_trajectories") as Tensor<float>;
        output.CompleteOperationsAndDownload();

        // Extract [1, 1, 20, 2] â†’ Vector2[20]
        Vector2[] futureTrajectory = new Vector2[20];
        for (int i = 0; i < 20; i++)
        {
            futureTrajectory[i] = new Vector2(output[i*2], output[i*2+1]);
        }

        return futureTrajectory;
    }
}
```

### 5.3 Camera Perception ì¶”ê°€

**í˜„ì¬**: Ground Truth Vector (242D)
**ëª©í‘œ**: Camera â†’ CNN â†’ feature vector â†’ RL

#### 5.3.1 ML-Agents CameraSensor ì„¤ì •

```csharp
// Assets/Scripts/Agents/E2EDrivingAgentCamera.cs (ì‹ ê·œ)

using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Sensors;

public class E2EDrivingAgentCamera : Agent
{
    public Camera frontCamera;

    public override void CollectObservations(VectorSensor sensor)
    {
        // ê¸°ì¡´ vector observations (242D)
        sensor.AddObservation(transform.position);  // 3D
        sensor.AddObservation(GetComponent<Rigidbody>().velocity);  // 3D
        // ... (ì´ 242D)
    }

    public override void Initialize()
    {
        // Add Camera Sensor (84x84 grayscale)
        var cameraSensorComponent = gameObject.AddComponent<CameraSensorComponent>();
        cameraSensorComponent.Camera = frontCamera;
        cameraSensorComponent.SensorName = "FrontCamera";
        cameraSensorComponent.Width = 84;
        cameraSensorComponent.Height = 84;
        cameraSensorComponent.Grayscale = true;  // 1 channel
        cameraSensorComponent.ObservationType = ObservationType.Default;
    }
}
```

**Camera ì„¤ì •**:
- Resolution: 84Ã—84 (Atari í‘œì¤€, DQN/PPOì—ì„œ ê²€ì¦ë¨)
- Grayscale: 1 channel (RGBëŠ” 3ë°° VRAM)
- FOV: 90ë„ (front camera)
- Position: Ego vehicle ì „ë©´ 1m, ë†’ì´ 1.5m

#### 5.3.2 CNN Encoder (Nature CNN)

```python
# python/src/models/planning/encoder.py (Level 2 ì¶”ê°€)

class NatureCNN(nn.Module):
    """
    Nature DQN CNN architecture
    Input: [batch, 84, 84, 1] (grayscale)
    Output: [batch, 512]
    """
    def __init__(self, input_shape=(84, 84, 1), output_dim=128):
        super().__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=8, stride=4),  # 84 â†’ 20
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # 20 â†’ 9
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # 9 â†’ 7
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 512),
            nn.ReLU(),
            nn.Linear(512, output_dim)
        )

    def forward(self, x):
        # x: [batch, 1, 84, 84]
        return self.conv(x)

# Fusion with vector observations
class MultimodalEncoder(nn.Module):
    def __init__(self, vector_dim=242, image_shape=(84, 84, 1), hidden_dim=256):
        super().__init__()

        # Vector encoder
        self.vector_encoder = nn.Sequential(
            nn.Linear(vector_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128)
        )

        # Image encoder
        self.image_encoder = NatureCNN(image_shape, output_dim=128)

        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(128 + 128, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, vector_obs, image_obs):
        vector_feat = self.vector_encoder(vector_obs)
        image_feat = self.image_encoder(image_obs)

        combined = torch.cat([vector_feat, image_feat], dim=-1)
        return self.fusion(combined)
```

**VRAM ì¶”ì •**:
- Input: 84Ã—84Ã—1 = 7,056 floats = 28 KB (ë¬´ì‹œ ê°€ëŠ¥)
- CNN params: (32Ã—8Ã—8 + 64Ã—4Ã—4Ã—32 + 64Ã—3Ã—3Ã—64 + 512Ã—3136 + 128Ã—512) Ã— 4 bytes â‰ˆ 7 MB
- Activations (batch=32): 32 Ã— (20Ã—20Ã—32 + 9Ã—9Ã—64 + 7Ã—7Ã—64 + 512) Ã— 4 bytes â‰ˆ 2 MB
- **Total**: ~10 MB (negligible)

**í•™ìŠµ ì†ë„ ì €í•˜**:
- Vector-only: ~5,000 steps/sec
- Vector+CNN: ~2,000 steps/sec (CNN forward pass ì¶”ê°€)
- í•™ìŠµ ì‹œê°„: 2-3ë°° ì¦ê°€

### 5.4 Route Planning ì¶”ê°€

**í˜„ì¬**: ê³ ì • waypoints
**ëª©í‘œ**: ë™ì  ê²½ë¡œ ê³„íš (A* on road graph)

#### 5.4.1 Road Graph êµ¬ì¡°

```csharp
// Assets/Scripts/Navigation/RoadGraph.cs (ì‹ ê·œ)

using System.Collections.Generic;
using UnityEngine;

[System.Serializable]
public class RoadNode
{
    public int id;
    public Vector3 position;
    public List<int> neighbors;  // Adjacent node IDs
    public float speedLimit;
    public string laneType;  // "straight", "left_turn", "right_turn"
}

[System.Serializable]
public class RoadEdge
{
    public int fromNodeId;
    public int toNodeId;
    public float cost;  // Distance or travel time
}

public class RoadGraph : MonoBehaviour
{
    public List<RoadNode> nodes;
    public List<RoadEdge> edges;

    private Dictionary<int, RoadNode> nodeDict;
    private Dictionary<int, List<RoadEdge>> adjacencyList;

    void Start()
    {
        BuildGraph();
    }

    void BuildGraph()
    {
        nodeDict = new Dictionary<int, RoadNode>();
        adjacencyList = new Dictionary<int, List<RoadEdge>>();

        foreach (var node in nodes)
        {
            nodeDict[node.id] = node;
            adjacencyList[node.id] = new List<RoadEdge>();
        }

        foreach (var edge in edges)
        {
            adjacencyList[edge.fromNodeId].Add(edge);
        }
    }

    public List<Vector3> FindPath(Vector3 start, Vector3 goal)
    {
        // Find nearest nodes
        int startNodeId = FindNearestNode(start);
        int goalNodeId = FindNearestNode(goal);

        // A* search
        var path = AStar(startNodeId, goalNodeId);

        // Convert node IDs to positions
        List<Vector3> waypoints = new List<Vector3>();
        foreach (int nodeId in path)
        {
            waypoints.Add(nodeDict[nodeId].position);
        }

        return waypoints;
    }

    private int FindNearestNode(Vector3 position)
    {
        int nearestId = -1;
        float minDist = float.MaxValue;

        foreach (var node in nodes)
        {
            float dist = Vector3.Distance(node.position, position);
            if (dist < minDist)
            {
                minDist = dist;
                nearestId = node.id;
            }
        }

        return nearestId;
    }

    private List<int> AStar(int startId, int goalId)
    {
        var openSet = new HashSet<int> { startId };
        var cameFrom = new Dictionary<int, int>();
        var gScore = new Dictionary<int, float> { [startId] = 0f };
        var fScore = new Dictionary<int, float> { [startId] = Heuristic(startId, goalId) };

        while (openSet.Count > 0)
        {
            int current = GetLowestFScore(openSet, fScore);

            if (current == goalId)
            {
                return ReconstructPath(cameFrom, current);
            }

            openSet.Remove(current);

            foreach (var edge in adjacencyList[current])
            {
                int neighbor = edge.toNodeId;
                float tentativeGScore = gScore[current] + edge.cost;

                if (!gScore.ContainsKey(neighbor) || tentativeGScore < gScore[neighbor])
                {
                    cameFrom[neighbor] = current;
                    gScore[neighbor] = tentativeGScore;
                    fScore[neighbor] = tentativeGScore + Heuristic(neighbor, goalId);

                    if (!openSet.Contains(neighbor))
                    {
                        openSet.Add(neighbor);
                    }
                }
            }
        }

        return new List<int>();  // No path found
    }

    private float Heuristic(int nodeId, int goalId)
    {
        return Vector3.Distance(nodeDict[nodeId].position, nodeDict[goalId].position);
    }

    private int GetLowestFScore(HashSet<int> openSet, Dictionary<int, float> fScore)
    {
        int lowest = -1;
        float minScore = float.MaxValue;

        foreach (int nodeId in openSet)
        {
            float score = fScore.ContainsKey(nodeId) ? fScore[nodeId] : float.MaxValue;
            if (score < minScore)
            {
                minScore = score;
                lowest = nodeId;
            }
        }

        return lowest;
    }

    private List<int> ReconstructPath(Dictionary<int, int> cameFrom, int current)
    {
        var path = new List<int> { current };

        while (cameFrom.ContainsKey(current))
        {
            current = cameFrom[current];
            path.Insert(0, current);
        }

        return path;
    }
}
```

#### 5.4.2 Unity Editor Tool (Graph ìƒì„±)

```csharp
// Assets/Scripts/Editor/RoadGraphEditor.cs (ì‹ ê·œ)

using UnityEngine;
using UnityEditor;

[CustomEditor(typeof(RoadGraph))]
public class RoadGraphEditor : Editor
{
    public override void OnInspectorGUI()
    {
        DrawDefaultInspector();

        RoadGraph graph = (RoadGraph)target;

        if (GUILayout.Button("Auto-Generate from Scene"))
        {
            AutoGenerateGraph(graph);
        }

        if (GUILayout.Button("Visualize Graph"))
        {
            VisualizeGraph(graph);
        }
    }

    void AutoGenerateGraph(RoadGraph graph)
    {
        // Find all road segments in scene
        var roadSegments = GameObject.FindGameObjectsWithTag("RoadSegment");

        graph.nodes = new List<RoadNode>();
        graph.edges = new List<RoadEdge>();

        int nodeId = 0;
        foreach (var segment in roadSegments)
        {
            // Extract waypoints from road mesh or spline
            var waypoints = segment.GetComponent<RoadWaypoints>();

            foreach (var wp in waypoints.points)
            {
                graph.nodes.Add(new RoadNode
                {
                    id = nodeId++,
                    position = wp,
                    neighbors = new List<int>(),
                    speedLimit = 15f,
                    laneType = "straight"
                });
            }
        }

        // Connect sequential nodes
        for (int i = 0; i < graph.nodes.Count - 1; i++)
        {
            float dist = Vector3.Distance(graph.nodes[i].position, graph.nodes[i+1].position);

            graph.edges.Add(new RoadEdge
            {
                fromNodeId = i,
                toNodeId = i + 1,
                cost = dist
            });

            graph.nodes[i].neighbors.Add(i + 1);
        }

        EditorUtility.SetDirty(graph);
    }

    void VisualizeGraph(RoadGraph graph)
    {
        foreach (var edge in graph.edges)
        {
            var from = graph.nodes.Find(n => n.id == edge.fromNodeId);
            var to = graph.nodes.Find(n => n.id == edge.toNodeId);

            Debug.DrawLine(from.position, to.position, Color.green, 5f);
        }
    }
}
```

**ì‚¬ìš©ë²•**:
1. ë„ë¡œ ì˜¤ë¸Œì íŠ¸ì— "RoadSegment" íƒœê·¸ ì¶”ê°€
2. RoadGraph ì»´í¬ë„ŒíŠ¸ ì¶”ê°€ í›„ "Auto-Generate from Scene" í´ë¦­
3. A* pathfinding ìë™ ë™ì‘

**ì„±ëŠ¥**: ìˆ˜ë°± ë…¸ë“œ ê¸°ì¤€ < 1ms (C# êµ¬í˜„)

---

## 6. TECH-SPEC.md ë§¤í•‘

### 6.1 í˜„ì¬ TECH-SPEC.mdì—ì„œ ì •ì˜í–ˆì§€ë§Œ êµ¬í˜„ë˜ì§€ ì•Šì€ í•­ëª©

| ì»´í¬ë„ŒíŠ¸ | TECH-SPEC ì„¹ì…˜ | ì •ì˜ ë‚´ìš© | Tesla FSD ëŒ€ì‘ | í˜„ì‹¤ì  êµ¬í˜„ |
|----------|--------------|----------|--------------|-----------|
| **BEVEncoder** | 3.4 Modular Encoder | "BEV representation for spatial reasoning" | RegNet + Transformer â†’ BEV 200Ã—200 | LSS ë‹¨ì¼ ì¹´ë©”ë¼ â†’ BEV 50Ã—50 |
| **TrajectoryPredictor** | 3.2 Prediction Module | "Constant Velocity or nuPlan Baseline" | Occupancy Flow (3D ConvGRU) | LSTM per-agent predictor |
| **ObservationEncoder Level 2** | 3.3.1 | "CNN for camera input" | RegNet-120GF Multi-scale | NatureCNN (84Ã—84) |
| **ObservationEncoder Level 3** | 3.3.1 | "BEV features" | Transformer fusion 8 cameras | LSS ë‹¨ì¼ ì¹´ë©”ë¼ |
| **ObservationEncoder Level 4** | 3.3.1 | "Temporal LSTM" | Occupancy Flow temporal | GRU on past observations |
| **GAIL Discriminator** | 3.3.3, 4.2 | "Human-likeness discriminator" | Fleet data GAN | nuPlan expert demo GAIL |
| **Trajectory Output** | ì—†ìŒ | ì—†ìŒ (ì§ì ‘ controlë§Œ) | MCTS + Neural Evaluator | Action space â†’ waypoints |

### 6.2 ê° í•­ëª©ì˜ TECH-SPEC ì •ì˜ vs Tesla ë°©ì‹ vs í˜„ì‹¤ì  êµ¬í˜„

#### BEVEncoder (Section 3.4)

**TECH-SPEC ì •ì˜**:
```python
# ì–¸ê¸‰ë§Œ ìˆê³  êµ¬í˜„ ì—†ìŒ
# "BEV features: optional (64D)"
```

**Tesla ë°©ì‹**:
- 8 cameras â†’ RegNet features â†’ Transformer Cross-Attention â†’ BEV 200Ã—200Ã—256
- ëª¨ë¸ í¬ê¸°: ~300M params
- ì¶”ë¡  ì‹œê°„: ~60ms (HW3 ìµœì í™”)

**í˜„ì‹¤ì  êµ¬í˜„** (RTX 4090):
```python
class LSS_BEV(nn.Module):
    """
    Lift-Splat-Shoot: ë‹¨ì¼ ì¹´ë©”ë¼ â†’ BEV
    Simplified from Tesla's 8-camera fusion
    """
    def __init__(self, camera_features_dim=128, bev_grid_size=(50, 50), bev_feature_dim=64):
        super().__init__()

        # Depth distribution predictor
        self.depth_net = nn.Sequential(
            nn.Linear(camera_features_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64)  # 64 depth bins
        )

        # BEV grid projection
        self.bev_grid_size = bev_grid_size
        self.bev_conv = nn.Sequential(
            nn.Conv2d(camera_features_dim, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, bev_feature_dim, 3, padding=1)
        )

    def forward(self, camera_features, camera_intrinsics):
        # camera_features: [batch, 128, 21, 21] (from CNN)
        # Output: [batch, 64, 50, 50] BEV features

        # 1. Predict depth distribution
        depth_dist = self.depth_net(camera_features.flatten(2).permute(0, 2, 1))
        depth_dist = torch.softmax(depth_dist, dim=-1)  # [batch, H*W, 64]

        # 2. Lift to 3D (frustum grid)
        # Simplified: use depth bins to create pseudo-3D
        # (ìƒëµ: ë³µì¡í•œ 3D projection ë¡œì§)

        # 3. Splat to BEV grid
        bev_features = self.bev_conv(camera_features)

        # 4. Resize to target grid size
        bev_features = F.interpolate(bev_features, size=self.bev_grid_size, mode='bilinear')

        return bev_features
```

**VRAM**: ~2GB (ë‹¨ì¼ ì¹´ë©”ë¼)
**ì¶”ë¡ **: ~15ms @ RTX 4090
**í•™ìŠµ ì‹œê°„**: 5-10M steps, 2-4ì£¼

#### TrajectoryPredictor (Section 3.2)

**TECH-SPEC ì •ì˜**:
```python
# python/src/models/prediction/predictor.py
class PredictionModule:
    def __init__(self, mode: str = "constant_velocity"):
        # Constant Velocity ë˜ëŠ” nuPlan Baseline ì–¸ê¸‰
```

**Tesla ë°©ì‹**:
- Occupancy Flow: past 3 frames â†’ future 2s
- 3D ConvGRU, per-voxel motion
- ëª¨ë¸ í¬ê¸°: ~50M params

**í˜„ì‹¤ì  êµ¬í˜„**:
- LSTM per-agent: 8 agents Ã— 2s horizon
- ëª¨ë¸ í¬ê¸°: ~5M params
- ì¶”ë¡ : ~5ms @ RTX 4090
- (Section 5.2ì—ì„œ ìƒì„¸ ì„¤ëª…)

#### GAIL Discriminator (Section 4.2)

**TECH-SPEC ì •ì˜**:
```yaml
# python/configs/planning/gail.yaml
gail:
  discriminator:
    hidden_layers: [256, 256]
```

**Tesla ë°©ì‹**:
- Human-likeness GAN: real driver trajectories vs model
- Discriminator input: trajectory sequence (2s)
- í•™ìŠµ ë°ì´í„°: Fleet Shadow Mode clips

**í˜„ì‹¤ì  êµ¬í˜„**:
```python
# python/src/models/planning/gail_discriminator.py

class GAILDiscriminator(nn.Module):
    """
    Discriminate expert (nuPlan) vs policy trajectories
    """
    def __init__(self, trajectory_dim=20, hidden_dim=256):
        super().__init__()

        # Trajectory encoder (RNN)
        self.encoder = nn.GRU(
            input_size=5,  # [x, y, v, heading, accel]
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True
        )

        # Binary classifier
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, trajectory):
        # trajectory: [batch, T, 5]
        _, hidden = self.encoder(trajectory)
        logits = self.classifier(hidden[-1])
        return logits  # P(expert)
```

**ë°ì´í„° ì†ŒìŠ¤**:
- Expert: nuPlan ì‹œë‚˜ë¦¬ì˜¤ (10k clips)
- Policy: RL í•™ìŠµ ì¤‘ ìƒì„±ëœ trajectory

**í•™ìŠµ**:
- ML-Agents GAIL êµ¬í˜„ í™œìš©
- Discriminator update: ë§¤ PPO epochë§ˆë‹¤ 2íšŒ
- ì˜ˆìƒ í•™ìŠµ: 3-5M steps, 1-2ì£¼

---

## 7. í•™ìˆ  E2E ì ‘ê·¼ë²• ì°¸ì¡°

### 7.1 UniAD (2023)

**ë…¼ë¬¸**: "Planning-oriented Autonomous Driving" (CVPR 2023)

**ì•„í‚¤í…ì²˜**:
```
Camera â†’ Backbone â†’ Query-based Multi-task Head
  â”œâ”€ Detection (DETR-style)
  â”œâ”€ Tracking (Track Queries)
  â”œâ”€ Mapping (Lane Queries)
  â”œâ”€ Motion (Trajectory Queries)
  â””â”€ Planning (Ego Query)
```

**í•µì‹¬ ê¸°ìˆ **:
- **Unified Query**: ë‹¨ì¼ Transformerë¡œ ëª¨ë“  Task ì²˜ë¦¬
- **Planning Query**: Ego vehicle futureë¥¼ ë‹¤ë¥¸ ê°ì²´ì™€ ë™ì¼í•˜ê²Œ ì˜ˆì¸¡
- **End-to-end Loss**: Detection + Tracking + Planning joint training

**ë³¸ í”„ë¡œì íŠ¸ ì ìš©**:
- Query-based multi-task êµ¬ì¡°ëŠ” RTX 4090ì—ì„œ ì‹¤í˜„ ê°€ëŠ¥
- Taskë³„ independent head ëŒ€ì‹  shared Transformer ì‚¬ìš©
- ë‹¨, camera ì…ë ¥ì€ ë‹¨ì¼ë¡œ ì¶•ì†Œ (8 cameras â†’ 1 camera)

**êµ¬í˜„ ë‚œì´ë„**: High
**VRAM**: ~12GB (ë‹¨ìˆœí™” ë²„ì „)
**í•™ìŠµ ì‹œê°„**: 10-20M steps, 4-8ì£¼

### 7.2 VAD (2024)

**ë…¼ë¬¸**: "Vectorized Scene Representation for Autonomous Driving" (CVPR 2024)

**ì•„í‚¤í…ì²˜**:
```
Camera â†’ CNN â†’ BEV â†’ Vectorization
  â”œâ”€ Lane: Polyline representation
  â”œâ”€ Agents: Vector (position, velocity, size)
  â””â”€ Planning: Bezier curve waypoints
```

**í•µì‹¬ ê¸°ìˆ **:
- **Vectorized Scene**: Raster(í”½ì…€) ëŒ€ì‹  Vector(ê¸°í•˜) í‘œí˜„
- **Efficiency**: VectorëŠ” memory-efficient (sparse)
- **Interpretability**: Waypointsê°€ ëª…ì‹œì  (visualizable)

**ë³¸ í”„ë¡œì íŠ¸ ì ìš©**:
- í˜„ì¬ observationì´ ì´ë¯¸ vector (242D) â†’ VADì™€ ì² í•™ì ìœ¼ë¡œ ìœ ì‚¬
- BEV raster â†’ vector conversion ì¶”ê°€ ê°€ëŠ¥
- Planning outputì„ Bezier curveë¡œ í‘œí˜„

**êµ¬í˜„ ë‚œì´ë„**: Medium
**ë³¸ í”„ë¡œì íŠ¸ í˜„ì¬ ìƒíƒœì™€ í˜¸í™˜ì„±**: Very High

**ì˜ˆì‹œ êµ¬í˜„**:
```python
class BezierTrajectoryCurve:
    """
    Bezier curve representation for smooth trajectory
    """
    def __init__(self, control_points):
        self.control_points = control_points  # [N, 2]

    def get_waypoints(self, num_samples=20):
        # Bezier interpolation
        t = torch.linspace(0, 1, num_samples)
        waypoints = []

        for t_i in t:
            # De Casteljau algorithm
            points = self.control_points.clone()
            while len(points) > 1:
                points = (1 - t_i) * points[:-1] + t_i * points[1:]
            waypoints.append(points[0])

        return torch.stack(waypoints)

# RL policy output: control points
action_space = [4, 2]  # 4 control points Ã— (x, y)
```

### 7.3 World Model (GAIA-1, DriveDreamer)

**GAIA-1** (Waymo, 2023):
- Video generation model: ê³¼ê±° í”„ë ˆì„ â†’ ë¯¸ë˜ í”„ë ˆì„ ìƒì„±
- 9B parameters (Transformer-based)
- ìš©ë„: Planning safety verification (ë¯¸ë˜ ì‹œë®¬ë ˆì´ì…˜)

**DriveDreamer** (2024):
- NeRF-based world model: Controllable scene generation
- Action-conditioned: steering â†’ ë¯¸ë˜ ì¥ë©´ ë³€í™”

**ë³¸ í”„ë¡œì íŠ¸ ì ìš©**:
- World Modelì€ Phase 7 (Advanced Topics)ì— ì í•©
- RTX 4090ìœ¼ë¡œëŠ” ì†Œê·œëª¨ ë²„ì „ë§Œ ê°€ëŠ¥ (Dreamer-v3 ìŠ¤íƒ€ì¼)
- 2D simplified: BEV grid future prediction

**êµ¬í˜„ ê°€ëŠ¥ì„±**:
- **ë¶ˆê°€ëŠ¥**: GAIA-1 ê·œëª¨ (9B params, video generation)
- **ì œí•œì  ê°€ëŠ¥**: Dreamer-v3 (2D BEV prediction, 50M params)

**Dreamer-v3 ê°„ëµ êµ¬ì¡°**:
```
Encoder: observation â†’ latent state (z)
Dynamics: z_t, action â†’ z_{t+1} (RNN)
Decoder: z â†’ reconstructed observation
Reward: z â†’ predicted reward
Actor: z â†’ action (RL policy)
```

**VRAM**: ~6GB
**í•™ìŠµ**: 5-10M steps, 2-4ì£¼
**ìš©ë„**: Model-based RL, ìƒ˜í”Œ íš¨ìœ¨ í–¥ìƒ

---

## 8. ê²°ë¡  ë° ìš°ì„ ìˆœìœ„

### 8.1 ì¦‰ì‹œ êµ¬í˜„ ê°€ëŠ¥ (Phase 5 ë‚´, 1-2ì£¼)

#### 1. Trajectory Output (ê°€ì¥ ì¤‘ìš”)
- **ëª©í‘œ**: Action spaceë¥¼ 2D control â†’ trajectory waypointsë¡œ ë³€ê²½
- **ì´ìœ **: Tesla FSDì™€ì˜ ê°€ì¥ í° gapì´ë©°, safety/interpretability í–¥ìƒ
- **êµ¬í˜„**: Section 5.1 ì°¸ì¡°
- **ìš°ì„ ìˆœìœ„**: **P0 (ìµœìš°ì„ )**
- **ì˜ˆìƒ ì‹œê°„**: 2-4ì‹œê°„ êµ¬í˜„ + 2-4M steps í•™ìŠµ = 1-2ì£¼
- **íŒŒì¼**:
  - `Assets/Scripts/Agents/E2EDrivingAgentTrajectory.cs` (Unity)
  - `Assets/Scripts/Controllers/TrajectoryTracker.cs` (PID controller)
  - `python/src/models/planning/trajectory_policy.py`
  - `python/configs/planning/trajectory_ppo.yaml`

#### 2. Route Planning (WaypointManager í™•ì¥)
- **ëª©í‘œ**: ê³ ì • waypoints â†’ ë™ì  A* pathfinding
- **ì´ìœ **: êµì°¨ë¡œ, lane change decisionì— í•„ìˆ˜
- **êµ¬í˜„**: Section 5.4 ì°¸ì¡°
- **ìš°ì„ ìˆœìœ„**: **P0**
- **ì˜ˆìƒ ì‹œê°„**: 1-2ì¼ êµ¬í˜„ + í…ŒìŠ¤íŠ¸
- **íŒŒì¼**:
  - `Assets/Scripts/Navigation/RoadGraph.cs`
  - `Assets/Scripts/Navigation/AStarPathfinder.cs`
  - `Assets/Scripts/Editor/RoadGraphEditor.cs`

### 8.2 ì¤‘ê¸° êµ¬í˜„ (Phase 6, 2-4ì£¼)

#### 3. Prediction Module (LSTM)
- **ëª©í‘œ**: Constant Velocity â†’ LSTM trajectory prediction
- **ì´ìœ **: ì°¨ì„  ë³€ê²½, braking ì˜ˆì¸¡ ê°€ëŠ¥
- **êµ¬í˜„**: Section 5.2 ì°¸ì¡°
- **ìš°ì„ ìˆœìœ„**: **P1**
- **ì˜ˆìƒ ì‹œê°„**: 3-5ì¼ êµ¬í˜„ + ë°ì´í„° ìˆ˜ì§‘ + í•™ìŠµ = 2ì£¼
- **íŒŒì¼**:
  - `Assets/Scripts/Data/TrajectoryRecorder.cs` (Unity ë°ì´í„° ìˆ˜ì§‘)
  - `python/src/models/prediction/lstm_predictor.py`
  - `python/src/training/train_prediction.py`
  - `Assets/Scripts/Inference/PredictionInference.cs`

#### 4. Camera Perception (CNN Encoder)
- **ëª©í‘œ**: Ground Truth â†’ Camera (84Ã—84) input
- **ì´ìœ **: ì‹¤ì œ ì„¼ì„œ ëª¨ë°©, Sim-to-Real ì¤€ë¹„
- **êµ¬í˜„**: Section 5.3 ì°¸ì¡°
- **ìš°ì„ ìˆœìœ„**: **P1**
- **ì˜ˆìƒ ì‹œê°„**: 1-2ì¼ êµ¬í˜„ + 5-10M steps = 2-4ì£¼
- **íŒŒì¼**:
  - `Assets/Scripts/Agents/E2EDrivingAgentCamera.cs`
  - `python/src/models/planning/encoder.py` (Level 2 ì¶”ê°€)
  - `python/configs/planning/camera_ppo.yaml`

### 8.3 ì¥ê¸° êµ¬í˜„ (Phase 7, 4-8ì£¼+)

#### 5. BEV Representation
- **ëª©í‘œ**: ë‹¨ì¼ ì¹´ë©”ë¼ â†’ BEV grid (50Ã—50)
- **ì´ìœ **: Spatial reasoning, ê³¡ì„ /êµì°¨ë¡œ ì´í•´
- **êµ¬í˜„**: LSS (Lift-Splat-Shoot) ë°©ì‹
- **ìš°ì„ ìˆœìœ„**: **P2 (ì—°êµ¬)**
- **ì˜ˆìƒ ì‹œê°„**: 1-2ì£¼ êµ¬í˜„ + 10-15M steps = 4-8ì£¼
- **íŒŒì¼**:
  - `python/src/models/perception/lss_bev.py`
  - `python/src/training/train_bev.py`

#### 6. World Model (Dreamer-v3)
- **ëª©í‘œ**: Model-based RL, ìƒ˜í”Œ íš¨ìœ¨ í–¥ìƒ
- **ì´ìœ **: í•™ìŠµ ì†ë„ 2-3ë°° í–¥ìƒ (ì´ë¡ ì )
- **ìš°ì„ ìˆœìœ„**: **P2 (ì‹¤í—˜ì )**
- **ì˜ˆìƒ ì‹œê°„**: 2-3ì£¼ êµ¬í˜„ + í•™ìŠµ
- **ì°¸ê³ **: DreamerV3 PyTorch êµ¬í˜„ í™œìš©

#### 7. GAIL/Hybrid RL+IL
- **ëª©í‘œ**: nuPlan expert demoë¡œ ì´ˆê¸°í™” â†’ RL fine-tuning
- **ì´ìœ **: í•™ìŠµ ì•ˆì •ì„±, human-likeness
- **ìš°ì„ ìˆœìœ„**: **P1-P2**
- **ì˜ˆìƒ ì‹œê°„**: 1ì£¼ ë°ì´í„° ì¤€ë¹„ + 3-5M steps = 2-3ì£¼
- **íŒŒì¼**:
  - `python/configs/planning/gail.yaml` (ê¸°ì¡´ í™œìš©)
  - `python/src/data/nuplan_expert.py` (expert demo ì¶”ì¶œ)

### 8.4 êµ¬í˜„ ìˆœì„œ ê¶Œì¥ (Gantt Chart)

```
Week 1-2:   [Trajectory Output] + [Route Planning]
            â””â”€> Stage 5B ì™„ë£Œ, A* pathfinding í†µí•©

Week 3-4:   [Prediction Module]
            â””â”€> LSTM predictor í•™ìŠµ, Unity í†µí•©

Week 5-8:   [Camera Perception] (optional, parallel)
            â””â”€> CNN encoder ì¶”ê°€, Level 2 í•™ìŠµ

Week 9-12:  [BEV Representation] (research phase)
            â””â”€> LSS êµ¬í˜„, BEV grid í•™ìŠµ

Week 13+:   [World Model] / [GAIL] (advanced)
            â””â”€> Dreamer-v3 ë˜ëŠ” GAIL ì‹¤í—˜
```

### 8.5 ìµœì¢… ê¶Œì¥ ì‚¬í•­

**ë‹¨ê¸° (1-2ì£¼)**:
1. **Trajectory Output ì¶”ê°€** â†’ ì¦‰ì‹œ ì‹œì‘ (P0)
2. **Route Planning ì¶”ê°€** â†’ ë³‘ë ¬ ì§„í–‰ (P0)

**ì¤‘ê¸° (2-4ì£¼)**:
3. **Prediction Module** â†’ Trajectory ì´í›„ (P1)
4. **Camera Perception** â†’ ì„ íƒì  (P1, ì‹¤í—˜ì )

**ì¥ê¸° (2-3ê°œì›”)**:
5. **BEV Representation** â†’ Phase 7 ì—°êµ¬ (P2)
6. **GAIL/World Model** â†’ ê³ ê¸‰ ì£¼ì œ (P2)

**í˜„ì‹¤ì  ëª©í‘œ**:
- Tesla FSD ìˆ˜ì¤€ ë‹¬ì„±ì€ ë¶ˆê°€ëŠ¥ (í•˜ë“œì›¨ì–´/ë°ì´í„° gap)
- **í•™ìˆ  ì—°êµ¬ ìˆ˜ì¤€ì˜ E2E Pipeline** êµ¬ì¶•ì€ ê°€ëŠ¥
- nuPlan benchmarkì—ì„œ **ìƒìœ„ 30-50% ì„±ëŠ¥** ëª©í‘œ
- RTX 4090 í™œìš© ê·¹ëŒ€í™”: **50-100M params** ëª¨ë¸ê¹Œì§€

**ì„±ê³µ ê¸°ì¤€**:
- Trajectory planning ì„±ê³µë¥ : > 80%
- nuPlan closed-loop score: > 50
- Collision rate: < 5%
- Inference latency: < 100ms

---

**Last Updated**: 2026-01-30
**Next Action**: Trajectory Output êµ¬í˜„ ì‹œì‘ (Section 5.1 ì½”ë“œ ì‘ì„±)
**Document Status**: Complete - Ready for Implementation
