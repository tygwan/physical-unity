# Physical AI í•™ìŠµ ë¡œë“œë§µ

Unity ML-Agentsë¥¼ í™œìš©í•œ Physical AI ê°œë°œì„ ìœ„í•œ ë‹¨ê³„ë³„ í•™ìŠµ ê°€ì´ë“œ

## Phase 1: ê¸°ì´ˆ ë‹¤ì§€ê¸° (1-2ì£¼)

### 1.1 Python ê¸°ì´ˆ
- [ ] Python ë¬¸ë²• ë³µìŠµ (í´ë˜ìŠ¤, ë°ì½”ë ˆì´í„°, íƒ€ì…íŒíŠ¸)
- [ ] NumPy ê¸°ë³¸ ì—°ì‚°
- [ ] PyTorch í…ì„œ ì¡°ì‘

**ì¶”ì²œ ìë£Œ:**
- [PyTorch 60ë¶„ ë¸”ë¦¬ì¸ ](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)

### 1.2 Unity ê¸°ì´ˆ
- [ ] Unity Editor ì¸í„°í˜ì´ìŠ¤ ìµíˆê¸°
- [ ] GameObject, Component ê°œë…
- [ ] C# ìŠ¤í¬ë¦½íŒ… ê¸°ì´ˆ
- [ ] **Rigidbody ë¬¼ë¦¬ ì‹œìŠ¤í…œ** (ì¤‘ìš”!)
  - AddForce, velocity, angularVelocity
  - FixedUpdate vs Update
- [ ] Colliderì™€ ì¶©ëŒ ê°ì§€

**ì¶”ì²œ ìë£Œ:**
- [Unity Learn - Beginner Scripting](https://learn.unity.com/project/beginner-scripting)

### 1.3 ìˆ˜í•™ ê¸°ì´ˆ
- [ ] ë²¡í„° ì—°ì‚° (ë‚´ì , ì™¸ì , ì •ê·œí™”)
- [ ] íšŒì „ (Quaternion ê¸°ì´ˆ)
- [ ] í™•ë¥  ë¶„í¬ (ì •ê·œë¶„í¬, ìƒ˜í”Œë§)

---

## Phase 2: ê°•í™”í•™ìŠµ ì´ë¡  (2-3ì£¼)

### 2.1 í•µì‹¬ ê°œë…
- [ ] MDP (Markov Decision Process)
- [ ] ìƒíƒœ(State), í–‰ë™(Action), ë³´ìƒ(Reward)
- [ ] ì •ì±…(Policy)ê³¼ ê°€ì¹˜í•¨ìˆ˜(Value Function)
- [ ] íƒí—˜ vs í™œìš© (Exploration vs Exploitation)

### 2.2 ì•Œê³ ë¦¬ì¦˜
- [ ] **PPO (Proximal Policy Optimization)** - ML-Agents ê¸°ë³¸
  - Clipped objective
  - Actor-Critic êµ¬ì¡°
- [ ] **SAC (Soft Actor-Critic)** - ì—°ì† í–‰ë™ ê³µê°„ì— ì í•©
- [ ] Reward Shaping ê¸°ë²•

**ì¶”ì²œ ìë£Œ:**
- [OpenAI Spinning Up](https://spinningup.openai.com/)
- [Lilian Weng's RL Blog](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)

---

## Phase 3: ML-Agents ì‹¤ìŠµ (2-3ì£¼)

### 3.1 ê¸°ë³¸ ì˜ˆì œ
- [ ] 3D Ball ì˜ˆì œ ì‹¤í–‰ ë° ë¶„ì„
- [ ] Push Block ì˜ˆì œ
- [ ] Walker ì˜ˆì œ (ë¡œë´‡ ë³´í–‰)

### 3.2 í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´í•´
```
Agent (C#)
â”œâ”€â”€ CollectObservations()   # ìƒíƒœ ìˆ˜ì§‘
â”œâ”€â”€ OnActionReceived()      # í–‰ë™ ì‹¤í–‰
â”œâ”€â”€ Heuristic()             # ìˆ˜ë™ ì œì–´
â””â”€â”€ OnEpisodeBegin()        # ì—í”¼ì†Œë“œ ì´ˆê¸°í™”
```

- [ ] Observation ì„¤ê³„
  - Vector Observation (ìœ„ì¹˜, ì†ë„ ë“±)
  - Ray Perception Sensor (ê±°ë¦¬ ê°ì§€)
  - Camera Sensor (ì´ë¯¸ì§€ ì…ë ¥)
- [ ] Action Space ì„¤ê³„
  - Discrete (ì´ì‚°) vs Continuous (ì—°ì†)
- [ ] Reward ì„¤ê³„
  - Sparse vs Dense Reward
  - ë³´ìƒ ìŠ¤ì¼€ì¼ë§

### 3.3 í•™ìŠµ ì„¤ì •
- [ ] trainer_config.yaml ì´í•´
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
  - batch_size, buffer_size
  - learning_rate
  - beta (ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜)
- [ ] TensorBoardë¡œ í•™ìŠµ ëª¨ë‹ˆí„°ë§

---

## Phase 4: Physical AI ì‹¬í™” (3-4ì£¼)

### 4.1 ë¡œë´‡ ì œì–´
- [ ] ê´€ì ˆ(Joint) ê¸°ë°˜ ë¡œë´‡ ëª¨ë¸ë§
  - Hinge Joint, Configurable Joint
- [ ] í† í¬ ì œì–´ vs ìœ„ì¹˜ ì œì–´
- [ ] ì—­ê¸°êµ¬í•™(IK) ê¸°ì´ˆ

### 4.2 ê³ ê¸‰ í•™ìŠµ ê¸°ë²•
- [ ] **Curriculum Learning**
  - ì‰¬ìš´ í™˜ê²½ â†’ ì–´ë ¤ìš´ í™˜ê²½
- [ ] **Imitation Learning**
  - GAIL (Generative Adversarial Imitation Learning)
  - Behavioral Cloning
- [ ] **Self-Play**
  - ê²½ìŸì  í™˜ê²½ í•™ìŠµ

### 4.3 Sim-to-Real
- [ ] Domain Randomization
  - ë¬¼ë¦¬ íŒŒë¼ë¯¸í„° ëœë¤í™”
  - ì‹œê°ì  ëœë¤í™”
- [ ] ì‹œë®¬ë ˆì´ì…˜ ì •í™•ë„ í–¥ìƒ

---

## Phase 5: í”„ë¡œì íŠ¸ ì ìš© (ì§„í–‰ ì¤‘)

### 5.1 ëª©í‘œ ì„¤ì •
- [ ] êµ¬ì²´ì ì¸ Physical AI ëª©í‘œ ì •ì˜
- [ ] í™˜ê²½ ì„¤ê³„
- [ ] ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„

### 5.2 ë°˜ë³µ ê°œë°œ
```
ì„¤ê³„ â†’ êµ¬í˜„ â†’ í•™ìŠµ â†’ í‰ê°€ â†’ ê°œì„  (ë°˜ë³µ)
```

---

## ì°¸ê³  ë¬¸ì„œ

### ê³µì‹ ë¬¸ì„œ
- [Unity ML-Agents Documentation](https://unity-technologies.github.io/ml-agents/)
- [ML-Agents GitHub](https://github.com/Unity-Technologies/ml-agents)

### ë…¼ë¬¸
- [PPO ë…¼ë¬¸](https://arxiv.org/abs/1707.06347)
- [SAC ë…¼ë¬¸](https://arxiv.org/abs/1801.01290)
- [Domain Randomization](https://arxiv.org/abs/1703.06907)

### ê°•ì˜
- [David Silver's RL Course](https://www.davidsilver.uk/teaching/)
- [CS285 Deep RL (Berkeley)](http://rail.eecs.berkeley.edu/deeprlcourse/)

---

## ì§„í–‰ ìƒí™© ì²´í¬

| Phase | ìƒíƒœ | ì‹œì‘ì¼ | ì™„ë£Œì¼ |
|-------|------|--------|--------|
| Phase 1: ê¸°ì´ˆ | ğŸ”„ ì§„í–‰ì¤‘ | - | - |
| Phase 2: RL ì´ë¡  | â³ ëŒ€ê¸° | - | - |
| Phase 3: ML-Agents | â³ ëŒ€ê¸° | - | - |
| Phase 4: ì‹¬í™” | â³ ëŒ€ê¸° | - | - |
| Phase 5: í”„ë¡œì íŠ¸ | â³ ëŒ€ê¸° | - | - |
