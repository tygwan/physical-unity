import type { Translations } from './en';

export const ko: Translations = {
  meta: {
    title: 'AD ML Platform | 자율주행 연구 포트폴리오',
    description:
      '자율주행 ML 플랫폼 - Unity + ML-Agents + PyTorch 기반 RL/IL 모션 플래닝 연구',
  },
  nav: {
    brand: 'ML 플랫폼',
    research: '연구',
    phases: '학습 단계',
    architecture: '아키텍처',
    insights: '인사이트',
    github: 'GitHub',
    navigation: '네비게이션',
    builtWith: '기술 스택',
  },
  footer: {
    description:
      '커리큘럼 기반 점진적 학습을 활용한 강화학습 자율주행 모션 플래닝 연구 플랫폼.',
    bottom: '자율주행 ML 플랫폼 -- RL/IL 모션 플래닝 연구',
  },
  hero: {
    badge: 'RL 모션 플래닝 연구',
    titleLine1: '자율주행',
    titleLine2: 'ML 플랫폼',
    subtitlePre: '7단계 점진적 커리큘럼 학습으로 ',
    subtitleHL1: 'PPO 강화학습',
    subtitleMid: ' 기반 End-to-End 모션 플래닝을 구현합니다. ',
    subtitleHL2: 'ISO 21448 (SOTIF)',
    subtitlePost: ' 안전 표준으로 검증.',
    viewResearch: '연구 보기',
    phaseRoadmap: '학습 로드맵',
    peakReward: '최고 보상',
    phasesComplete: '완료 단계',
    collisionTarget: '충돌률 목표',
    safetyBenchmark: '안전 기준',
    failedAttempts: '실패 시도',
    overallProgress: '전체 진행률',
    complete: '완료',
    inProgress: '진행 중',
    failed: '실패',
  },
  stats: {
    peakReward: '최고 보상',
    peakRewardSub: 'Phase I: 곡선도로+NPC',
    phasesComplete: '완료 단계',
    failedAttempts: '실패 시도',
    observationSpace: '관측 공간',
    observationSub: 'ego + 경로 + NPC + 차선',
    parallelEnvs: '병렬 환경',
    parallelSub: '동시 학습',
    collisionRate: '충돌률',
    collisionSub: '안전 목표',
    policiesDiscovered: '발견된 정책',
    policiesSub: '시행착오 기반',
  },
  research: {
    heading: '연구',
    sub: '반복적 학습에서 도출된 안전 프레임워크, 경쟁 분석, 정책 발견',
    sotifTitle: 'SOTIF 프레임워크 (ISO 21448)',
    sotifDesc:
      'ISO 21448은 의도된 기능의 안전성(SOTIF)을 정의합니다 -- 시스템 결함이 아닌 기능적 한계에서 발생하는 위험을 다룹니다. 4사분면 모델은 지식과 안전 상태에 따라 시나리오를 분류합니다.',
    sotifGoal:
      '목표: 사분면 2와 3(안전하지 않은 영역)을 축소하고 사분면 1(알려진 안전)을 확장. 각 학습 단계는 체계적으로 시나리오를 Unknown에서 Known으로 이동시킵니다.',
    known: '알려진',
    unknown: '미지의',
    teslaTitle: 'Tesla FSD 격차 분석',
    teslaDesc:
      'Tesla FSD 12/13 아키텍처와의 컴포넌트별 비교. 이 프로젝트는 상용 제품이 아닌 연구 플랫폼입니다 -- 격차는 의도적이며 유익합니다.',
    vs: 'vs',
    thComponent: '컴포넌트',
    thTesla: 'Tesla FSD',
    thProject: '이 프로젝트',
    thGap: '격차',
    thFeasibility: '실현가능성',
    teslaInsightLabel: '핵심 인사이트:',
    teslaInsight:
      'Tesla FSD는 4M+ 차량과 Dojo 슈퍼컴퓨터(1.1 ExaFLOPS)를 갖춘 상용 제품입니다. 이 프로젝트는 단일 RTX 4090(82.6 TFLOPS)으로 운영 -- 1,300만 배의 컴퓨팅 격차. 가치는 알고리즘 검증에 있으며, 규모 복제가 아닙니다. Vehicle Control만이 직접 RL 정책 출력을 통해 동등 수준을 달성했습니다.',
    policyTitle: '정책 발견 로그',
    policyDesc:
      '시행착오 학습에서 발견된 설계 원칙. 각 실패는 확립된 표준과 수렴하는 실행 가능한 정책을 생산했습니다.',
    verified: '검증됨',
    policyInProgress: '진행 중',
    planned: '계획됨',
    fail: '실패',
    fix: '수정',
    matchingStandard: '대응 표준:',
    referencesTitle: '참고문헌 및 학술 연구',
    referencesDesc:
      '학습 커리큘럼, 보상 설계, 안전 검증 프레임워크에 근거가 되는 표준, 논문, 기술 분석.',
    openLink: '열기',
    convergenceInsightLabel: '수렴 인사이트:',
    convergenceInsight:
      '시행착오 학습에서 발견된 경험적 정책(P-001~P-011)은 확립된 국제 안전 표준과 독립적으로 수렴합니다. P-002(단계적 커리큘럼)는 SOTIF의 점진적 복잡성 원칙에 대응합니다. P-005/P-006(계획됨)은 UN R157/R171 수치 제한을 직접 구현합니다. 이 수렴은 RL 기반 정책 학습이 자연스럽게 안전 공학 원칙을 재발견함을 검증합니다.',
  },
  phases: {
    heading: '학습 단계',
    sub: '7단계에 걸친 점진적 커리큘럼 학습. 실패한 시도는 분기 경로로 표시 -- 각 실패는 이후 단계에서 재사용되는 설계 정책을 생산했습니다.',
    rewardEvolution: '보상 변화 (전체 시도)',
    success: '성공',
    failed: '실패',
    inProgressLabel: '진행 중',
    active: '활성',
    phaseDetails: '단계 상세',
    failureCause: '실패 원인',
    keyInsight: '핵심 인사이트',
    obs: '관측',
    steps: '스텝',
    training: '학습 중...',
  },
  architecture: {
    heading: '아키텍처',
    sub: '시스템 설계, 학습 파이프라인, 하드웨어 사양',
    trainingPipeline: '학습 파이프라인',
    thisProject: '이 프로젝트',
    teslaFsd: 'Tesla FSD 12/13',
    hardwareEnv: '하드웨어 환경',
    simulation: '시뮬레이션',
    framework: '프레임워크',
    mlEngine: 'ML 엔진',
    rlAlgorithm: 'RL 알고리즘',
    middleware: '미들웨어',
    languages: '언어',
    compute: '컴퓨팅',
    monitoring: '모니터링',
    observation: '관측',
    observationValue: '280D 벡터',
    observationDesc: 'ego 상태, 경로, NPC, 차선, 교차로, 신호, 보행자',
    actionSpace: '행동 공간',
    actionValue: '연속 2D',
    actionDesc: '가속 [-4, 2] m/s2, 조향 [-0.5, 0.5] rad',
    reward: '보상',
    rewardValue: '7개 구성요소',
    rewardDesc: '진행, 속도, 차선, 추월, 위반, 저크, 시간',
    trainingLabel: '학습',
    trainingValue: '16 병렬',
    trainingDesc: '커리큘럼 학습과 동시 환경',
    unitySimulation: 'Unity 시뮬레이션',
    parallelEnvs: '16 병렬 환경',
    mlAgentsBridge: 'ML-Agents 브릿지',
    grpcProtocol: 'gRPC 프로토콜',
    pytorchPpo: 'PyTorch PPO',
    gpuTraining: 'GPU 학습',
    onnxExport: 'ONNX 내보내기',
    modelPackaging: '모델 패키징',
    unityInference: 'Unity 추론',
    realtimeControl: '실시간 제어',
    input: '입력',
    perception: '인지',
    prediction: '예측',
    planning: '계획',
    control: '제어',
  },
  insights: {
    heading: '인사이트',
    sub: '7단계 RL 학습에서 얻은 교훈 -- 성공한 것, 실패한 것, 그리고 실패가 안전 표준과 수렴하는 과정',
    whatWorked: '성공한 것',
    whatFailed: '실패한 것',
    denseReward: 'Dense Reward Shaping',
    denseRewardDetail:
      '7개 보상 구성요소(진행, 속도, 차선 유지, 추월, 위반, 저크, 시간)가 초기부터 명확한 학습 신호를 제공했습니다.',
    curriculum: '커리큘럼 학습',
    curriculumDetail:
      '점진적 난이도 증가(NPC, 속도 구간, 목표 거리)로 안정적 학습을 가능하게 했습니다. 단계적 임계값이 동시 전환을 방지했습니다.',
    checkpoint: '체크포인트 전이',
    checkpointDetail:
      '이전 단계에서의 Warm-start가 핵심적인 부트스트랩을 제공했습니다. Phase B v2는 v1(처음부터 학습)이 실패한 곳에서 Phase A 체크포인트로 성공했습니다.',
    parallel: '16 병렬 환경',
    parallelDetail:
      '16개 영역에서의 동시 학습이 다양한 경험과 빠른 수렴을 제공했습니다. PPO on-policy 학습에 필수적입니다.',
    curriculumShock: '동시 커리큘럼 충격',
    curriculumShockDetail:
      'Phase D v1: 동일 임계값(보상 ~400)에서 3개 파라미터가 동시 전환, 보상이 +406에서 -4,825로 폭락. v2에서 단계적 임계값으로 수정.',
    freshStart: '복잡한 관측으로 처음부터 학습',
    freshStartDetail:
      'Phase B v1: NPC 상호작용과 함께 처음부터 학습하여 1.8M 스텝에서 붕괴. 복잡한 관측은 단순한 정책에서의 Warm-start가 필요.',
    excessivePenalty: '과도한 페널티 설계',
    excessivePenaltyDetail:
      'Phase B v1: speedUnderPenalty -0.1/step이 에이전트에게 완전 정지를 학습시킴. v2에서 -0.02(80% 감소)로 수정하여 학습 복원.',
    obsDimChange: '관측 차원 변경',
    obsDimChangeDetail:
      '242D에서 254D로의 전환이 체크포인트 호환성을 깨뜨림. 차선 관측 통합을 위해 처음부터 학습이 필요, 축적된 지식 손실.',
    convergenceTitle: '경험적 발견에서 표준으로의 수렴',
    convergenceDesc:
      '시행착오에서 발견된 정책들이 확립된 안전 표준 및 모범 사례와 독립적으로 수렴합니다.',
    standardsTitle: '안전 표준 통합',
    sotifStandard: 'SOTIF (ISO 21448)',
    sotifStandardDesc:
      '4사분면 안전 모델. 각 관측 차원에 대한 기능적 불충분성 분석. 체계적 Known/Unknown Safe/Unsafe 분류.',
    unR171: 'UN R171 (DCAS)',
    unR171Desc:
      'Cut-in/Cut-out 테스트 파라미터: TTC 1.5-5.0s, 최대 감속 -7.0 m/s2, 저크 제한 3.0 m/s3. Level 2 운전 보조 검증.',
    unR157: 'UN R157 (ALKS)',
    unR157Desc:
      '횡가속도 제한 0.3g, 곡률 변화율 dk/ds < 0.1/m2, 횡방향 오차 < 0.3m. Level 3 자동 차선 유지 표준.',
  },
};
