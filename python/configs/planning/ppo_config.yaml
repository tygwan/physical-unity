# PPO Training Configuration for E2E Driving Model
# Usage: python -m src.training.train_rl --config configs/planning/ppo_config.yaml

model:
  level: 1
  hidden_dims: [512, 512, 256]
  activation: relu
  dropout: 0.0  # No dropout for RL
  predict_trajectory: false
  total_obs_dim: 238
  action_dim: 2

ppo:
  # Core PPO
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5

  # Training
  total_steps: 2000000
  num_steps: 2048        # Steps per rollout
  num_epochs: 10         # Epochs per PPO update
  batch_size: 256
  lr: 3.0e-4
  lr_schedule: linear    # linear, cosine, constant

  # Reward
  reward_scale: 1.0
  reward_clip: 10.0

environment:
  type: unity            # unity or dummy
  port: 5004
  num_envs: 1
  time_scale: 20         # Speed up simulation

output:
  dir: experiments/rl/ppo_v1
  log_interval: 10
  save_interval: 50
  eval_interval: 20
  eval_episodes: 5

# CIMRL (Hybrid BC + RL)
cimrl:
  enabled: false
  pretrained_path: null  # Path to BC checkpoint
  # When enabled:
  # pretrained_path: experiments/bc/best.pt
  rl_lr_multiplier: 0.1  # Lower LR for fine-tuning
  freeze_encoder_epochs: 5  # Freeze encoder initially
