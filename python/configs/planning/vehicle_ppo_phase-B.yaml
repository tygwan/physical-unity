# Phase B: Decision Learning - PPO Configuration
# Focuses on validating and enhancing overtaking decision-making
# Created: 2026-01-28

behaviors:
  E2EDrivingAgent:
    trainer_type: ppo
    
    hyperparameters:
      batch_size: 4096
      buffer_size: 40960
      learning_rate: 3e-4
      learning_rate_schedule: constant
      beta: 5e-3
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      shared_critic: false
      tau: 0.005
      time_horizon: 2048
      max_steps: 3000000
      summary_freq: 50000
      checkpoint_interval: 500000
      normalize: true
      use_recurrent: false
      memory_size: 256
      sequence_length: 64
      vis_encode_type: nature_cnn
    
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 3
      vis_encode_type: nature_cnn
    
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
        network_settings:
          normalize: false
          hidden_units: 128
          num_layers: 2
    
    # INITIALIZATION: Phase 0 checkpoint (conservative approach)
    init_path_override: results/phase-0-foundation/E2EDrivingAgent/E2EDrivingAgent-8000047.pt
    
    keep_checkpoints: 5
    max_steps: 3000000

# CURRICULUM: Progressive overtaking difficulty (4 stages)
curriculum:
  overtaking_difficulty:
    description: "Progressive overtaking challenge - from simple baseline to complex multi-vehicle scenarios"
    config:
      
      # Stage 0: Baseline - No obstacles (750K steps)
      # Objective: Establish speed/progress reward baselines
      lesson_0:
        config:
          num_active_npcs: 0
          npc_speed_range: [0, 0]
          min_required_overtakes: 0
          overtaking_bonus_multiplier: 1.0
          speed_reward_weight: 0.3
          lane_center_weight: 0.2
          following_penalty_enabled: false
        completion_criteria:
          max_steps: 750000
          measure: reward
          threshold: 800.0
          require_reset: true
      
      # Stage 1: Single slow NPC - Forced overtaking (750K-1500K steps)
      # Objective: Agent learns overtaking behavior with clear incentive
      lesson_1:
        config:
          num_active_npcs: 1
          npc_speed_range: [8.0, 8.0]  # Single slow NPC (8 m/s)
          min_required_overtakes: 1     # MUST overtake to progress
          overtaking_bonus_multiplier: 1.5
          overtaking_bonus: 5.0          # +5.0 per successful overtake
          speed_reward_weight: 0.3
          lane_center_weight: 0.2
          following_penalty_enabled: true
          following_penalty: -0.5        # Penalty when TTC < 5s
        completion_criteria:
          max_steps: 1500000
          measure: reward
          threshold: 1100.0
          require_reset: true
      
      # Stage 2: Two mixed-speed NPCs - Selective overtaking (1500K-2250K steps)
      # Objective: Agent learns WHEN to overtake (decision-making)
      lesson_2:
        config:
          num_active_npcs: 2
          npc_speed_range: [8.0, 15.0]  # Mixed speeds: one slow, one fast
          min_required_overtakes: 1      # Selective overtaking
          overtaking_bonus_multiplier: 2.0
          overtaking_bonus: 5.0
          speed_reward_weight: 0.3
          lane_center_weight: 0.2
          following_penalty_enabled: true
          following_penalty: -0.5
        completion_criteria:
          max_steps: 2250000
          measure: reward
          threshold: 1400.0
          require_reset: true
      
      # Stage 3: Dense multi-vehicle - Complex scenarios (2250K-3000K steps)
      # Objective: Complex navigation with multiple overtaking decisions
      lesson_3:
        config:
          num_active_npcs: 4
          npc_speed_range: [6.0, 16.0]  # Variable speeds
          min_required_overtakes: 2
          overtaking_bonus_multiplier: 2.5
          overtaking_bonus: 5.0
          speed_reward_weight: 0.3
          lane_center_weight: 0.2
          following_penalty_enabled: true
          following_penalty: -0.5
        completion_criteria:
          max_steps: 3000000
          measure: reward
          threshold: 1500.0
          require_reset: false

# REWARD STRUCTURE (Phase B - Decision Learning Focus)
reward_structure:
  progress:
    weight: 1.0
    description: "Progress towards goal (unchanged from Phase A)"
  
  goal_reached:
    weight: 10.0
    description: "Bonus for reaching goal (unchanged)"
  
  speed_reward:
    weight: 0.3
    description: "Speed tracking reward (REDUCED from 0.5 to 40%)"
    target_speed: 18.0
    max_speed: 20.0
  
  lane_center:
    weight: 0.2
    description: "NEW: Reward for center lane positioning"
    purpose: "Encourages agent to center before overtaking"
  
  overtaking_bonus:
    weight: 5.0
    description: "NEW: Increased overtaking bonus (INCREASED from 3.0)"
    purpose: "Incentivize explicit overtaking decisions"
  
  following_penalty:
    weight: -0.5
    description: "NEW: Penalty for following behind slow NPC"
    trigger_condition: "TTC < 5 seconds"
    purpose: "Creates urgency to make overtaking decision"
  
  collision_penalty:
    weight: -10.0
    description: "Collision penalty (unchanged)"
  
  near_collision_penalty:
    weight: -2.0
    description: "Near-collision penalty (unchanged)"
    ttc_threshold: 2.0

environment_settings:
  inference_type: local
  side_channels: []

phase_metadata:
  phase_name: "Phase B: Decision Learning"
  version: "phase-B"
  created_date: "2026-01-28"
  description: "Validate and enhance overtaking decision-making with rebalanced rewards and progressive curriculum"
  
  initialization_strategy: "Phase 0 checkpoint (conservative, unbiased approach)"
  
  success_targets:
    mean_reward: "+1500-1800"
    overtaking_success_rate: ">70%"
    overtaking_events: ">150"
    collision_rate: "<5%"
    goal_completion: ">90%"
  
  key_changes_from_phase_a:
    - "Rebalanced speed reward (0.5 -> 0.3 per step)"
    - "Added lane-center reward (0.2 for middle positioning)"
    - "Added following penalty (-0.5 when TTC < 5s)"
    - "Increased overtaking bonus (3.0 -> 5.0)"
    - "Progressive curriculum with explicit overtaking requirements"
    - "Enhanced logging for overtaking detection validation"
    - "Initialize from Phase 0 (clean slate) not Phase A"
  
  critical_validation_milestones:
    - "750K steps: Baseline performance (expect +800 reward)"
    - "1.5M steps: Single NPC overtaking learned (expect >50% success)"
    - "2.25M steps: Mixed-speed decisions validated (expect >70% success)"
    - "3.0M steps: Complex scenario mastery (expect +1500+ reward)"
  
  rollback_conditions:
    - "Mean reward < +1200 after 2M steps"
    - "Collision rate > 8% (safety violation)"
    - "Overtaking events = 0 after Stage 1 complete"
    - "Unstable training (oscillation >500 reward for 500K steps)"
    - "Curriculum not progressing to next stage"
  
  contingency_plans:
    - "If overtaking detection still 0: Implement explicit lane-change logging"
    - "If speed performance drops >15%: Revert weight to 0.5, increase penalty"
    - "If collision rate >5%: Increase penalties (-15 collision, -5 near-collision)"
    - "If training stalls >500K steps: Increase learning rate to 5e-4"
