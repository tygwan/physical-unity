# E2EDrivingAgent PPO Phase L v4: P-027 Speed Reward Farming Fix
# Strategy: RESUME from L v1 12.5M checkpoint (best stable model)
#
# Phase L v1: Crosswalk yield farming (+20K) -> P-026 yield cap
# Phase L v2: Slow-driving farming near crosswalk -> P-026 v3 crosswalkStopTimer
# Phase L v3: Speed reward farming (+8000, 22K-98K step episodes) -> P-027
#
# P-027 Code Fixes (in E2EDrivingAgent.cs):
#   1. speedRewardAccumulated cap: 200 per episode
#   2. timePenalty: -0.001 -> -0.05 (50x increase)
#   3. MaxStep = 3000 (hard per-episode limit)
#   4. Goal bypass detection: EndEpisode if agent Z > goal Z + 20m
#
# Reward scale change:
#   Old: speed uncapped (~8000), time -0.001/step, no step limit -> 20K+ total
#   New: speed capped (200), time -0.05/step, 3000 step limit -> ~300-400 total
#   Curriculum thresholds adjusted accordingly (600/620 -> 200/230)
#
# P-019 compliance: No Unicode special characters in YAML

behaviors:
  E2EDrivingAgent:
    trainer_type: ppo

    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 1.0e-4    # Lower LR for resume fine-tuning
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: constant
      beta_schedule: constant
      epsilon_schedule: constant

    network_settings:
      normalize: false
      hidden_units: 512
      num_layers: 3

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0

    init_path: results/phase-L-v1/E2EDrivingAgent/E2EDrivingAgent-12499788.pt
    max_steps: 5000000       # 5M additional steps
    time_horizon: 256
    summary_freq: 10000
    keep_checkpoints: 10
    checkpoint_interval: 500000
    threaded: false

environment_parameters:
  # All driving params at final values (already learned in v1)
  goal_distance:
    curriculum:
      - name: Full
        value: 230.0

  num_lanes:
    curriculum:
      - name: TwoLanes
        value: 2.0

  center_line_enabled:
    curriculum:
      - name: CenterLineOn
        value: 1.0

  num_active_npcs:
    curriculum:
      - name: ThreeNPCs
        value: 3.0

  npc_speed_ratio:
    curriculum:
      - name: NormalNPCs
        value: 0.85

  npc_speed_variation:
    curriculum:
      - name: FullVariation
        value: 0.15

  intersection_type:
    curriculum:
      - name: YJunction
        value: 3.0

  turn_direction:
    curriculum:
      - name: RightTurn
        value: 2.0

  traffic_signal_enabled:
    curriculum:
      - name: SignalActive
        value: 1.0

  signal_green_ratio:
    curriculum:
      - name: HardGreen
        value: 0.4

  curve_direction_variation:
    curriculum:
      - name: RandomCurves
        value: 0.5

  road_curvature:
    curriculum:
      - name: MediumCurve
        value: 0.5

  speed_zone_count:
    curriculum:
      - name: OneZone
        value: 1.0

  # Pedestrian curriculum: gradual introduction
  # Thresholds lowered for P-027 reward scale (max ~350 vs old ~700+)
  num_pedestrians:
    curriculum:
      - name: NoPedestrians
        completion_criteria:
          measure: reward
          behavior: E2EDrivingAgent
          signal_smoothing: true
          min_lesson_length: 3000
          threshold: 200.0
        value: 0.0
      - name: OnePedestrian
        completion_criteria:
          measure: reward
          behavior: E2EDrivingAgent
          signal_smoothing: true
          min_lesson_length: 3000
          threshold: 230.0
        value: 1.0
      - name: TwoPedestrians
        value: 2.0

# =======================================================
# BUILD-BASED MULTI-ENV TRAINING
# IMPORTANT: PhaseL.exe rebuilt with P-027 anti-farming fix
# =======================================================

env_settings:
  env_path: Builds/PhaseL/PhaseL.exe
  num_envs: 3
  base_port: 5010
  timeout_wait: 600

engine_settings:
  width: 640
  height: 480
  quality_level: 1
  time_scale: 20
  target_frame_rate: -1
  capture_frame_rate: 60
  no_graphics: true

torch_settings:
  device: cuda
