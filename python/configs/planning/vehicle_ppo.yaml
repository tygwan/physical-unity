# E2EDrivingAgent PPO Training Configuration
# Usage: mlagents-learn python/configs/planning/vehicle_ppo.yaml --run-id=driving_ppo_v1
#
# Observation: 238D vector (ego 8D + history 40D + agents 160D + route 30D)
# Action: 2 continuous (steering [-1,1], acceleration [-1,1])
# Reward: composite (progress, safety, comfort, goal)

behaviors:
  E2EDrivingAgent:
    trainer_type: ppo

    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 3.0e-4
      beta: 5.0e-3              # Entropy regularization
      epsilon: 0.2              # PPO clipping
      lambd: 0.95               # GAE lambda
      num_epoch: 5              # More epochs for complex driving
      learning_rate_schedule: linear

    network_settings:
      normalize: false          # Obs already normalized in agent code
      hidden_units: 512
      num_layers: 3

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0

    # Training settings
    max_steps: 2000000
    time_horizon: 256           # Longer horizon for driving
    summary_freq: 5000
    threaded: false

# Environment settings
env_settings:
  env_path: null               # null = Unity Editor
  num_envs: 1
  base_port: 5004
  timeout_wait: 300

# Checkpoint settings
checkpoint_settings:
  run_id: driving_ppo_v1
  resume: false
  force: false

# Engine configuration
engine_settings:
  width: 640
  height: 480
  quality_level: 1
  time_scale: 20               # 20x speed for training
  target_frame_rate: -1
  capture_frame_rate: 60
  no_graphics: false

# TensorBoard settings
torch_settings:
  device: null  # Auto-detect (fixes normalize + CUDA mismatch)
